pr-02 spec: dataset loaders v1

goal

add dataset loaders/adapters that expose a stable internal interface for:
	•	pretrain text (fineweb-edu, wikitext, roam passthrough hook, gutenberg)
	•	sft chat (databricks dolly-15k, openassistant oasst1)

fineweb-edu must support huggingface streaming (streaming=True) and “take N samples” without materializing the dataset.  ￼
oasst1 must be reconstructed from a message tree using message_id/parent_id, with roles alternating prompter/assistant.  ￼

non-goals
	•	no tokenization
	•	no caching token ids
	•	no packing/truncation to T
	•	no training loop changes
	•	no model changes
	•	no network-hitting unit tests (use a separate smoke script)

repo contracts (must not change)
	•	role vocabulary: system | user | assistant
	•	chat serialization is handled elsewhere (pr-04). pr-02 outputs raw messages, not special tokens.

⸻

public interfaces (single source of truth)

create data/types.py:

from dataclasses import dataclass
from typing import Iterator, Literal

Role = Literal["system", "user", "assistant"]

@dataclass(frozen=True)
class PretrainSample:
    text: str
    source: str
    meta: dict

@dataclass(frozen=True)
class ChatMessage:
    role: Role
    content: str

@dataclass(frozen=True)
class ChatSample:
    messages: list[ChatMessage]
    source: str
    meta: dict

loader modules + signatures:

data/fineweb_edu.py

from typing import Iterator, Optional
from .types import PretrainSample

def iter_fineweb_edu(
    *,
    name: str = "CC-MAIN-2024-10",
    split: str = "train",
    streaming: bool = True,
    shuffle: bool = True,
    shuffle_buffer_size: int = 10_000,
    seed: int = 42,
    take: Optional[int] = None,
) -> Iterator[PretrainSample]:
    """
    yields PretrainSample(text=..., source="fineweb_edu", meta=...)
    requirements:
      - must use datasets.load_dataset(..., streaming=True) by default
      - if shuffle=True, must use iterable_dataset.shuffle(seed=..., buffer_size=...)
      - if take is not None, must yield at most take samples
    field selection rule:
      - prefer sample["text"]
      - else: pick the first string field in the sample dict
      - else: raise ValueError with sample keys shown
    """

fineweb-edu usage example exists on the dataset card.  ￼

data/wikitext.py

(you already have this in v0; keep it, but adapt to PretrainSample too.)

def iter_wikitext(
    *,
    config: str = "wikitext-103-raw-v1",
    split: str = "train",
    take: int | None = None,
) -> Iterator[PretrainSample]:
    """non-streaming ok; yields text lines as samples; skips empty strings."""

data/dolly.py

from typing import Iterator, Optional
from .types import ChatSample

def iter_dolly_sft(
    *,
    split: str = "train",
    seed: int = 42,
    shuffle: bool = True,
    take: Optional[int] = None,
    include_system: bool = False,
) -> Iterator[ChatSample]:
    """
    loads databricks/databricks-dolly-15k.
    mapping:
      - if include_system: add system msg "you are a helpful assistant."
      - user content:
          instruction
          + if context non-empty: "\\n\\ncontext:\\n{context}"
      - assistant content: response
    meta must include: category (if present), and original row index/id if available.
    """

(dolly is an instruction-following dataset created by databricks employees; you’re using it as human-written sft.)  ￼

default: include_system=False.

data/oasst1.py

from typing import Iterator, Optional
from .types import ChatSample

def iter_oasst1_sft(
    *,
    split: str = "train",
    seed: int = 42,
    shuffle_trees: bool = True,
    take_trees: Optional[int] = None,
    english_only: bool = True,
    max_messages: int = 32,
) -> Iterator[ChatSample]:
    """
    loads OpenAssistant/oasst1 and reconstructs chat threads.

    reconstruction policy (locked):
      - build all root->leaf threads per message_tree_id (cap by max_messages)
      - if english_only: filter messages where lang == "en" (if field exists)
      - map roles:
          oasst role "prompter" -> "user"
          oasst role "assistant" -> "assistant"
      - do NOT invent system messages.

    meta must include: message_tree_id, leaf_message_id, and list of message_ids in the thread.
    """

oasst1 is explicitly a message-tree dataset, reconstructable via parent_id/message_id, and roles are assistant vs prompter.  ￼

implementation constraints:
	•	must not do quadratic scans. build dicts:
	•	children[parent_id] -> list[child_id]
	•	msg[message_id] -> record
	•	reconstruct by:
	1.	find roots (parent_id is null) grouped by message_tree_id
	2.	dfs to leaves, emitting threads
	•	shuffle at tree level if shuffle_trees=True (shuffle list of tree ids using python random.Random(seed); this PR is allowed to use python random for ordering since training determinism is handled elsewhere)

data/gutenberg.py

support two dataset ids because the raw one can be awkward:
	•	default: nikolina-p/gutenberg_clean_en_splits (english-only cleaned subset)  ￼
	•	fallback/optional: manu/project_gutenberg (raw project gutenberg)  ￼

from typing import Iterator, Optional
from .types import PretrainSample

def iter_gutenberg(
    *,
    dataset_id: str = "nikolina-p/gutenberg_clean_en_splits",
    split: str = "train",
    streaming: bool = True,
    shuffle: bool = True,
    shuffle_buffer_size: int = 10_000,
    seed: int = 42,
    take: Optional[int] = None,
) -> Iterator[PretrainSample]:
    """
    yields book texts as PretrainSample.

    field selection rule:
      - prefer "text"
      - else first string field
      - else error
    """


⸻

file layout constraints

allowed to add/modify only:
	•	data/types.py
	•	data/fineweb_edu.py
	•	data/wikitext.py
	•	data/dolly.py
	•	data/oasst1.py
	•	data/gutenberg.py
	•	scripts/smoke_loaders.py
	•	tests/test_dolly_mapping.py
	•	tests/test_oasst1_reconstruction.py
	•	tests/test_fineweb_field_selection.py (fixture-only)

no other files.

⸻

tests (must be unit, no network)

tests/test_dolly_mapping.py
	•	build a tiny fake dolly row dict with instruction/context/response/category
	•	assert output ChatSample.messages has expected 2 messages (or 3 if include_system=True)
	•	assert context formatting matches exactly

tests/test_oasst1_reconstruction.py
	•	create a tiny synthetic message tree (hardcoded list of dicts) with:
	•	root prompter → assistant → prompter → assistant (two branches)
	•	run internal reconstruction helper (must be factored to accept records list)
	•	assert:
	•	all emitted threads alternate user/assistant
	•	max_messages is enforced
	•	meta contains message ids

tests/test_fineweb_field_selection.py
	•	pass sample dicts into a select_text_field(sample) helper:
	•	sample with text
	•	sample with content
	•	sample with no string fields → raises with keys included

⸻

smoke script (network ok, manual)

scripts/smoke_loaders.py must:
	•	print 2 samples from each loader with take=2
	•	for oasst1: print first 1–2 reconstructed threads, showing roles + first 80 chars
	•	exit non-zero on exception
	•	be runnable as python -m scripts.smoke_loaders

⸻

acceptance checklist

pr is done when:
	•	pytest passes locally without network
	•	python -m scripts.smoke_loaders prints samples for:
	•	fineweb-edu streaming (name="CC-MAIN-2024-10")  ￼
	•	dolly-15k  ￼
	•	oasst1  ￼
	•	gutenberg_clean_en_splits  ￼
	•	wikitext
