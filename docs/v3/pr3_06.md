# pr-06 spec — fix “triple dropout” by centralizing dropout in submodules

## goal

remove redundant dropout application in `Block` so dropout is applied **exactly once per sublayer** (attention + mlp), while preserving:
- training behavior (still regularizes)
- tracing correctness (returned attention probs are **pre-dropout**)
- public forward APIs and shapes

this pr reduces pointless stochasticity and over-regularization in a tiny model.

---

## hard constraints

- do not change architecture semantics beyond dropout placement (no new layers, norms, activations, attention variants)
- do not touch tokenizer, datasets, caching, optimizer, schedules
- do not change `GPT.forward(x)` signature or outputs shape
- do not change `GPT.forward_with_attn_trace(...)` public behavior (still returns pre-dropout attention probabilities when tracing)
- keep manual attention math (no `scaled_dot_product_attention`)
- training must still have dropout when `cfg.dropout > 0` and model is in train mode
- eval (`model.eval()`) must remain deterministic

---

## problem statement (what’s wrong today)

current implementation applies dropout in three places for attention:
1) attention probs dropout (`attn_dropout`)
2) attention output dropout (`resid_dropout`)
3) block-level residual dropout (`Block.dropout(attn_out)`)

and for mlp:
- block-level residual dropout (`Block.dropout(mlp_out)`)
(mlp may or may not have its own internal dropout; this PR must make dropout policy explicit.)

this is sloppy: it increases noise, can degrade learning in small models, and makes behavior harder to reason about.

---

## target dropout policy (locked)

### attention sublayer
- keep both:
  - `attn_dropout` on attention probabilities (after softmax)
  - `resid_dropout` on projected attention output (after output projection)
- **remove** any additional block-level dropout wrapping attention residual add

### mlp sublayer
- apply dropout **inside the mlp module**, not in `Block`
- required: `MLP` must have `resid_dropout` applied to its output (after the second linear/projection)
- **remove** any block-level dropout wrapping mlp residual add

### block-level residual adds
- `Block.forward` must do:
  - `x = x + attn_out`
  - `x = x + mlp_out`
  with **no extra dropout wrapper**

---

## code changes

### files allowed to modify/create

you may modify:
- `niels_gpt/model/blocks.py`

you may create:
- `tests/test_dropout_policy.py`

do not modify other files.

---

## implementation requirements

### 1) remove block-level dropout wrapper

in `Block.__init__`:
- delete `self.dropout = nn.Dropout(cfg.dropout)` or replace with `nn.Identity()` (preferred: delete)

in `Block.forward`:
- remove `self.dropout(...)` calls entirely

current pattern (bad):
```py
x = x + self.dropout(attn_out)
x = x + self.dropout(self.mlp(...))

required pattern:

x = x + attn_out
x = x + mlp_out

2) ensure mlp has its own output dropout

in the MLP module (wherever defined in blocks.py):
	•	add self.resid_dropout = nn.Dropout(cfg.dropout)
	•	apply it to the mlp output (after second linear/proj)

required:

out = self.fc2(hidden)
out = self.resid_dropout(out)
return out

notes:
	•	if the mlp already has dropout, you must not double-apply. enforce a single canonical resid_dropout for mlp output.
	•	do not add extra dropout elsewhere in mlp unless it already exists; keep it minimal.

3) tracing must remain pre-dropout

when return_attn=True, the returned attn_probs must be the pre-dropout softmax probabilities (sum to 1 along keys dim), not attn_dropped.

this must hold in both:
	•	normal forward attention
	•	kv-cache prefill and decode_step paths (if they return attn rows)

⸻

acceptance tests

create tests/test_dropout_policy.py with:

test 1: structural dropout policy

instantiate a tiny config with dropout=0.1 and assert:
	•	Block has no attribute dropout or it is nn.Identity
	•	CausalSelfAttention has attn_dropout and resid_dropout that are nn.Dropout
	•	MLP has resid_dropout that is nn.Dropout

(this catches regressions where someone reintroduces block-level dropout.)

test 2: dropout is active in train mode
	•	create a single Block (or a tiny GPT with 1 layer) with dropout=0.5
	•	set model.train()
	•	feed fixed input tensor x
	•	run with two different seeds and assert outputs differ

pseudo:

torch.manual_seed(0); y0 = block(x)
torch.manual_seed(1); y1 = block(x)
assert not torch.allclose(y0, y1)

(this ensures dropout still exists somewhere.)

test 3: attention probs returned are pre-dropout and normalized
	•	set dropout=0.9 (aggressive) and model.train()
	•	run attention with return_attn=True
	•	assert returned attn_probs:
	•	are finite
	•	sum to 1 along last dim within tolerance (1e-4)
	•	have values in [0,1] within eps

this proves tracing returns pre-dropout probs even in train mode.

⸻

done means
	•	block-level dropout wrapper is removed (no triple dropout)
	•	mlp output dropout is applied exactly once inside mlp
	•	attention tracing still returns pre-dropout softmax probs
	•	tests pass
