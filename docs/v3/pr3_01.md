# pr-01 spec — canonical source keys + fail-fast cache validation (break compat)

## goal

make data source naming and cache resolution unambiguous and fail-fast.

- break backwards compatibility: **`wikitext` is the only valid key** (no `wiki` alias).
- enforce that `mix_pretrain` / `mix_sft` keys are valid and sum to 1.
- validate required cache directories/files exist **before** training starts.
- print a short “resolved data plan” (sources + sizes) at job start.

if code disagrees with this spec, code is wrong.

## hard constraints

- do not change model architecture, tokenizer behavior, training loop math, optimizer, or loss.
- do not modify the cache on disk (no auto-renames, no migrations, no aliasing).
- do not add new data sources in this PR (primer-as-sft is later).
- no “helpful fallback.” errors must be early, loud, and specific.

## non-goals

- adding primer to build-all or sft (later PRs)
- changing eval semantics (later PR)
- changing CLI behavior (later PR)

---

## canonical source keys (locked)

### pretrain sources
valid pretrain cache source keys (directory names under `data/cache/pretrain/`):

- `wikitext`
- `fineweb_edu`
- `gutenberg`
- `roam`

### sft sources
valid sft cache source keys (directory names under `data/cache/sft/`):

- `dolly15k`
- `oasst1`

(note: `primer` will be added as an sft source in pr-03; not in pr-01)

---

## config schema behavior (locked)

### mix constraints
for `data.mix_pretrain` and `data.mix_sft`:

- keys must be a subset of the valid source keys for that phase
- values must be floats in (0, 1]
- probabilities must sum to 1.0 within tolerance `1e-6`
- empty mixes are invalid

### defaults
update defaults so they are valid given the canonical keys.

- any default that currently uses `"wiki"` must be replaced with `"wikitext"`.

### error messages
errors should be actionable and mention:
- the invalid keys (if any)
- the valid key set
- for missing caches: exact missing paths

---

## cache layout assumptions (locked)

### pretrain token cache layout
`{pretrain_token_cache}/{source}/meta.json` must exist
`{pretrain_token_cache}/{source}/{split}/` must exist for split in `{train,val}`

### sft token cache layout
`{sft_token_cache}/{source}/meta.json` must exist
`{sft_token_cache}/{source}/{split}/` must exist for split in `{train,val}`

no other files are required to exist at validation time.

---

## required new helper functions

create `niels_gpt/train/cache_validate.py` (or `niels_gpt/train/validate_caches.py`; pick ONE and keep it) containing:

```python
from __future__ import annotations
from dataclasses import dataclass
from pathlib import Path
from typing import Iterable

@dataclass(frozen=True)
class CacheRequirement:
    source: str
    split: str  # "train" or "val"
    meta_path: Path
    split_dir: Path

def list_requirements(cache_dir: Path, sources: Iterable[str], *, splits: Iterable[str]) -> list[CacheRequirement]:
    """pure: returns required paths for each (source, split)."""

def validate_token_caches(cache_dir: Path, sources: Iterable[str], *, splits: Iterable[str] = ("train","val")) -> None:
    """
    raises FileNotFoundError with a multi-line message listing ALL missing paths.
    does not partially succeed: either everything exists or it raises.
    """


⸻

where validation must run (locked)

pretrain jobs

in the pretrain entrypoint (wherever job config is built and just before training starts):
	•	validate data.mix_pretrain keys
	•	call validate_token_caches(pretrain_cache_dir, sources=mix_pretrain.keys(), splits=("train","val"))

sft jobs

in the sft entrypoint:
	•	validate data.mix_sft keys
	•	call validate_token_caches(sft_cache_dir, sources=mix_sft.keys(), splits=("train","val"))

do this before constructing datasets/mixtures and definitely before the first training step.

⸻

resolved data plan log (required)

at the start of each job (pretrain + sft), print a short summary:
	•	phase name: pretrain or sft
	•	cache dir root
	•	for each source:
	•	source name
	•	split directories present (train/val)
	•	if meta.json contains a token count field (unknown schema), try to print it:
	•	best-effort only: if missing or unreadable, print tokens=? and continue
	•	example format (not strict):
	•	data plan (pretrain): cache=data/cache/pretrain sources=[wikitext:tokens=..., roam:tokens=?]

do not parse shard files; only read meta.json if it’s small and exists.

⸻

tests (must exist)

create tests/test_cache_validation.py:
	1.	invalid key rejected (pretrain)
	•	build settings with mix_pretrain={"wiki": 1.0}
	•	assert settings validation raises ValueError mentioning wiki and wikitext
	2.	probabilities must sum to 1
	•	mix_pretrain={"wikitext": 0.9, "roam": 0.2} raises ValueError
	3.	missing cache paths listed
	•	in a tmpdir with no files, call validate_token_caches(tmp/pretrain, ["wikitext"])
	•	assert FileNotFoundError message contains both:
	•	.../wikitext/meta.json
	•	.../wikitext/train
	•	.../wikitext/val
	4.	valid cache passes
	•	create minimal directory structure with empty meta.json and train/val dirs
	•	assert no exception

tests must not require network access or real caches.

⸻

files allowed to change

you may create:
	•	niels_gpt/train/cache_validate.py
	•	tests/test_cache_validation.py

you may modify:
	•	niels_gpt/settings.py (and any config defaults in the same module)
	•	the pretrain entrypoint module (where _load_sources is called) to invoke validation early
	•	the sft entrypoint module (where _load_sft_sources is called) to invoke validation early

do not modify other files.
