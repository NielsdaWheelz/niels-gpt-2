# pr-03 spec — primer as SFT dataset (jsonl → token+label cache)

single source of truth for this PR. if code disagrees with this doc, code is wrong.

## goal

add **primer** as a first-class **SFT source**:
- accept `data/primer.jsonl` (chat messages)
- render each example using the repo’s special tokens (`<|sys|> <|usr|> <|asst|> <|eot|>`)
- build an **SFT token cache** under `data/cache/sft/primer/...` in the same sharded `.bin + meta.json` format as existing SFT caches
- enable training/eval to include `"primer"` in `mix_sft` and (by default) include it in the default SFT mixture

## hard constraints

- do not change model architecture, training loss math, optimizer, or attention code
- do not modify pretrain caching or pretrain runner behavior
- do not introduce new datasets beyond primer
- do not change tokenizer training or special token strings
- primer must be treated as **SFT** (assistant-only loss via label masking)
- must be deterministic given `seed`
- must fail fast with clear errors if primer file is missing or malformed

## non-goals

- no packing multiple examples into one sequence (one example → one sequence for v1)
- no “structure vs voice” splitting (single dataset)
- no curriculum scheduling or weight tuning
- no web/server/streaming features
- no changes to pr-01/pr-02 key enforcement (assume canonical keys already enforced; this PR just uses `"primer"`)

---

## definitions

### special tokens (already defined elsewhere; must match repo)
- `<|sys|>`
- `<|usr|>`
- `<|asst|>`
- `<|eot|>`

### roles
primer messages use roles:
- `system`
- `user`
- `assistant`

### model context length
- `T_max = model_cfg.T`

### sft labels
- `labels[i] = token_id` only for positions that should contribute to loss
- `labels[i] = -100` (ignore_index) elsewhere

---

## primer.jsonl format (locked)

path: `data/primer.jsonl`

each line is one training example:
```json
{"messages":[{"role":"system","content":"..."},{"role":"user","content":"..."},{"role":"assistant","content":"..."}]}

rules:
	•	messages must be a non-empty list
	•	each message is an object with exactly keys: role, content
	•	role must be one of: system, user, assistant
	•	content must be a string (may be empty, but discouraged)
	•	file is UTF-8

examples may contain multiple user/assistant turns.

⸻

chat template rendering (locked)

rendering function

create a single canonical function that turns messages into token ids and labels:

def render_chat_sft(
    tokenizer: "SentencePieceTokenizer",
    messages: list[dict],
    *,
    add_final_eot: bool = True,
) -> tuple[list[int], list[int]]:
    """
    returns:
      input_ids: list[int]
      labels:    list[int] (same length), with -100 masking

    template:
      <|sys|> {system_content} <|eot|>
      <|usr|> {user_content} <|eot|>
      <|asst|> {assistant_content} <|eot|>
      ... repeated for each message in order

    labeling rule:
      - tokens belonging to assistant content AND the assistant <|eot|> token are labeled (loss-active)
      - everything else is -100

    notes:
      - the special tokens themselves are produced by inserting their token IDs explicitly (not via text pieces)
      - content is tokenized with tokenizer.encode(content)
    """

role-to-special mapping (locked)
	•	system   → <|sys|>
	•	user     → <|usr|>
	•	assistant→ <|asst|>

eot placement (locked)
	•	append <|eot|> after every message (including assistant)
	•	include assistant <|eot|> in loss-active labels so the model learns to stop

malformed message sequences
	•	if an example contains no assistant messages: raise ValueError
	•	if the first message is not system or user: allow, but still render in order (no reordering)
	•	do not enforce alternation beyond requiring at least one assistant message

⸻

truncation/padding policy (locked)

SFT examples must be truncated to fit T_max.

policy:
	•	if rendered input_ids length > T_max:
	•	keep the last T_max tokens (right-truncate the left prefix)
	•	apply the same truncation to labels
	•	do NOT pad here (padding is handled by batcher if it exists)
	•	after truncation, require at least 2 tokens (else drop example)

rationale:
	•	simplest; keeps the most recent turns and the assistant response tail.

⸻

dataset split (locked)

split by example (line), not by tokens.

parameters:
	•	val_frac = settings.data.primer.val_frac (default 0.10 if not present)
	•	seed = settings.training.seed (single seed source)

algorithm:
	•	load all examples
	•	deterministic shuffle using random.Random(seed)
	•	take first n_val = floor(N * val_frac) as val, rest train

invariants:
	•	train and val sets are disjoint
	•	split is deterministic for fixed seed and file content

⸻

caching format (locked)

output location
	•	root: settings.data.caches.sft_token_cache (default data/cache/sft)
	•	source name: primer
	•	splits: train, val

paths:
	•	data/cache/sft/primer/meta.json
	•	data/cache/sft/primer/train/shard_00000.bin …
	•	data/cache/sft/primer/val/shard_00000.bin …

shard binary layout (must match existing SFT cache conventions)

use the same binary format and helper utilities used for dolly/oasst1 SFT caches.
(do not invent a new format.)

if the repo uses:
	•	a flat stream of int32/int16 token IDs with per-example offsets
	•	or per-example length-prefixed records
	•	or parallel input_ids.bin and labels.bin
then primer must match that exactly.

implementation directive:
	•	reuse the existing “SFT cache writer” used by dolly/oasst1.
	•	primer builder must call the same writer with input_ids and labels.

meta.json contents (minimum required)

meta must include at least:
	•	source: “primer”
	•	tokenizer_sha256: from tokenizer settings
	•	special_token_ids: dict of ids for sys/usr/asst/eot
	•	splits: {train: {…}, val: {…}} with:
	•	num_examples
	•	num_tokens (input_ids count)
	•	num_label_tokens (count of labels != -100)
	•	num_shards
	•	shard_size (or list)
	•	val_frac, seed
	•	T_max used for truncation

⸻

CLI wiring (in this PR)

extend niels_gpt/cache/cli.py:
	•	add command: build-sft-primer

usage:

python -m niels_gpt.cache.cli build-sft-primer \
  --primer-jsonl data/primer.jsonl \
  --out-dir data/cache/sft \
  --seed 42 \
  --val-frac 0.1 \
  --t-max 1024

rules:
	•	default values should come from settings when available
	•	must print a short summary:
	•	num examples train/val
	•	num tokens train/val
	•	output dir paths

do not modify build-all in this PR (that’s pr-04).

⸻

training integration (in this PR)
	•	ensure "primer" is allowed in SFT source names and can be loaded by the existing SFT loader
	•	update default mix_sft to include "primer" by default (weight value can be small but must be non-zero)

locked default mix_sft
in settings.py default factory:

{"primer": 0.10, "oasst1": 0.70, "dolly15k": 0.20}

(you may adjust exact values only if required by existing defaults/tests; keep primer non-zero.)

⸻

acceptance tests (must exist)

create tests/test_primer_sft.py:
	1.	render + masking correctness

	•	build a tiny fake tokenizer stub (or use the real tokenizer if lightweight) with known special token ids
	•	render messages:
	•	system, user, assistant
	•	assert:
	•	input_ids and labels same length
	•	labels are -100 everywhere except assistant span + assistant eot
	•	assistant eot is labeled

	2.	deterministic split

	•	create temp primer.jsonl with N examples
	•	run split twice with same seed; indices must match exactly

	3.	cache build smoke

	•	run build function into temp dir
	•	assert meta.json exists and includes required fields
	•	assert train/val shard dirs exist and contain >=1 shard file

⸻

done means
	•	python -m niels_gpt.cache.cli build-sft-primer ... produces a valid SFT cache at data/cache/sft/primer/...
	•	SFT training can include "primer" as a source without special casing
	•	default SFT mix includes primer
	•	all tests pass
