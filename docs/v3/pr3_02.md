# pr-02 spec — sft eval defaults + dual-metric reporting

single source of truth for this pr. if code disagrees with this, code is wrong.

## goal

make sft validation evaluate **sft data by default**, not wikitext. ensure training logs/reporting make it impossible to confuse “pretrain val loss” with “sft val loss”.

## hard constraints

- do not change model architecture, tokenizer, caches, dataset contents, or training objectives
- do not change how batches are constructed beyond selecting which validation batch_fn/eval_fn to use
- do not change checkpoint format or selection logic in this pr (best-by metric remains whatever it is today)
- no new external dependencies
- keep `evaluate_pretrain(...)` and `evaluate_sft(...)` signatures unchanged

## scope

### in scope
- settings defaults: `val_sft_source` default must become `"sft"`
- validation source selection must be explicit and unambiguous
- logging/output must include **both**:
  - `val_pretrain_loss` (wikitext val, via pretrain-style batch_fn)
  - `val_sft_loss` (sft val mixture, via sft-style batch_fn + ignore_index)

### out of scope
- adding new eval metrics (perplexity, benchmarks, behavioral tests)
- changing “best checkpoint” criterion
- changing data mixes/weights
- changing cache building or introducing primer as sft (handled in later PR)

---

## definitions

### sources
- **pretrain val**: always `wikitext` validation stream/caches (existing mechanism)
- **sft val**: held-out SFT split(s) from SFT sources (existing `_load_sft_sources(... split="val")`)

### `val_sft_source` meaning (locked)
`settings.data.val_sft_source` is an enum-like string:

- `"sft"`: evaluate on sft val (calls `evaluate_sft`)
- `"wikitext"`: evaluate on wikitext val using pretrain evaluator (calls `evaluate_pretrain`)

default must be `"sft"`.

### reporting keys (locked)
- `val_pretrain_loss`: float (average cross-entropy loss) on wikitext val
- `val_sft_loss`: float (average cross-entropy loss) on sft val, computed by `evaluate_sft(... ignore_index=-100)`

if one is not computed at a given step, it must be logged as `None` (or omitted consistently) — choose one and be consistent.
default behavior in this PR: compute both at every eval step.

---

## required behavior

### 1) settings change
in `niels_gpt/settings.py`:

- change `DataSettings.val_sft_source` default from `"wikitext"` to `"sft"`.
- validate allowed values: only `"sft"` or `"wikitext"` (raise ValueError otherwise).
  - (if pr-01 already has validators/enums, extend them rather than duplicating logic.)

### 2) sft training runner eval behavior
in `train/sft.py` (or wherever the SFT loop lives):

- on eval cadence (`step_num % train_cfg.eval_every == 0`), compute:

  1. **pretrain validation**
     - use existing `_load_wikitext_val(streams_cache_dir, T=model_cfg.T, ...)` logic
     - build a `batch_fn` for that stream (existing pretrain batching approach)
     - call `evaluate_pretrain(model, batch_fn=..., eval_batches=train_cfg.eval_batches)` (or existing eval_batches param)
     - record as `val_pretrain_loss`

  2. **sft validation** (controlled by `val_source_choice = settings.data.val_sft_source`)
     - if `val_source_choice == "sft"`:
       - load sft val sources via `_load_sft_sources(cache_dir, sft_sources, split="val", ...)`
       - build `SFTMixture` (existing)
       - call `evaluate_sft(model, batch_fn=..., eval_batches=train_cfg.eval_batches, ignore_index=-100)`
       - record as `val_sft_loss`
     - else if `"wikitext"`:
       - do NOT call `evaluate_sft`
       - set `val_sft_loss = None` (or omit consistently)

- **important:** never label a pretrain eval result as `val_sft_loss`. names must match the metric type.

### 3) logging/output
ensure the training loop prints/logs both metrics with the locked keys.

example (format not locked, keys are):
- `step=200 train_loss=... val_pretrain_loss=... val_sft_loss=...`

if you have JSON logging, ensure these keys appear in the dict.

---

## acceptance tests (must exist)

create `tests/test_sft_eval_defaults.py` (or add to an existing tests file if you already have a training-config test module).

1) **default is sft**
- instantiate settings with defaults
- assert `settings.data.val_sft_source == "sft"`

2) **invalid setting rejected**
- set `val_sft_source="oops"`
- assert settings construction raises ValueError

3) **runner chooses evaluate_sft when val_sft_source == "sft"**
- unit test using monkeypatch/mocks:
  - monkeypatch `train.eval.evaluate_sft` to record calls
  - monkeypatch `train.eval.evaluate_pretrain` to record calls
  - run a minimal eval tick function (factor out eval block if needed)
  - assert:
    - `evaluate_pretrain` called (for val_pretrain_loss)
    - `evaluate_sft` called (for val_sft_loss)

4) **runner does not call evaluate_sft when val_sft_source == "wikitext"**
- set val_sft_source to "wikitext"
- assert evaluate_sft not called, evaluate_pretrain called, and val_sft_loss is None (or omitted consistently)

tests must not require downloading datasets or building real caches; use mocks/fakes for loaders.

---

## done means

- default settings run SFT training with `val_sft_source="sft"`
- eval step produces two clearly named losses: `val_pretrain_loss` and `val_sft_loss`
- tests prove:
  - default behavior
  - invalid values rejected
  - evaluator selection works
- no changes to training objective or checkpoint format
