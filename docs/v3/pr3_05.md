# pr-05 spec — chat cli uses kv-cache generation by default

## goal

make the terminal chat demo **fast by default** by switching the chat cli to use the existing **kv-cache** generation path. preserve current CLI UX and output formatting, but generation must stop **only** on the `<|eot|>` token id (no substring stop hacks).

## hard constraints

- do not modify training code paths (datasets, batching, optimizer, loss, trainers)
- do not change tokenizer training or tokenizer files
- do not change model architecture
- do not add a web server / sse / ui
- must use existing kv-cache implementations:
  - `niels_gpt.infer.kv_cache.allocate_kv_cache`
  - `niels_gpt.infer.kv_cache.prefill`
  - `niels_gpt.infer.kv_cache.decode_step`
  - `niels_gpt.generate.generate_ids_greedy_cached` and/or `niels_gpt.generate.generate_ids_cached`
- stopping behavior: **token-based only**
  - stop when generated token id == `eot_token_id`
  - no scanning for `"\nuser:"` etc.
- keep CLI deterministic options working if present (seed / generator), but do not add new RNG systems

## non-goals

- streaming tokens to the terminal
- changing prompt formatting, personas, or chat template semantics
- changing sampling strategy defaults
- performance micro-optimizations beyond using kv-cache

---

## current behavior (baseline)

`niels_gpt/chat_cli.py` currently imports and calls `generate_text(...)` which uses the **uncached** `generate_ids(...)` loop (full forward per token).

pr-05 changes this so the CLI uses the cached path by default.

---

## required behavior

### 1) default to kv-cache path

- by default, `chat_cli.py` must call a cached generation function.
- add a flag to disable kv-cache for debugging:

–no-kv-cache

default: kv-cache enabled.

### 2) generation function selection

Use the existing generation API:

- prefer:
  - `generate_ids_cached(...)` for sampling (temperature/top-k) **and** optional tracing support (even if unused by CLI)
- optionally, allow a `--greedy` flag if you already have it; if greedy exists in the CLI today, keep it working by calling:
  - `generate_ids_greedy_cached(...)`

If the CLI currently supports sampling parameters (temperature, top_k, max_new_tokens), these must be passed through unchanged to the cached generator.

### 3) enforce eot-only stopping

The cached generation already stops on `eot_token_id`. The CLI must:

- obtain `eot_token_id` from the tokenizer special token ids
- pass it into the cached generator
- **never** use substring stopping logic

If any older stop logic exists in `generate_text(...)` or chat_cli, the CLI must not rely on it.

### 4) preserve CLI output semantics

- the CLI must still:
  - format the prompt using the existing chat template code (whatever it currently does)
  - decode generated ids to text using the existing tokenizer
  - extract and print only the assistant’s reply (if the CLI currently does assistant-reply extraction)

Important: **do not change** how prompts are built or how replies are extracted, except for replacing any substring-based stopping with `<|eot|>` stop.

---

## implementation plan (tight)

### A) update chat_cli to call cached generation

In `niels_gpt/chat_cli.py`:

- add argparse option:
  - `--no-kv-cache` (store_true)
- when kv-cache is enabled, call the cached path:
  - if CLI is greedy → `generate_ids_greedy_cached`
  - else → `generate_ids_cached`

### B) optional: add a cached text wrapper

If `chat_cli.py` is structured around `generate_text(...)`, you may add in `niels_gpt/generate.py`:

```python
@torch.no_grad()
def generate_text_cached(...) -> str:
    """
    wrapper that:
      - formats prompt_text (already formatted by caller or not; match existing conventions)
      - encodes to ids
      - calls generate_ids_cached with eot_token_id
      - decodes ids to text
      - returns decoded text
    """

BUT: do not refactor large call graphs. Prefer modifying chat_cli call site directly.

⸻

acceptance tests

Add tests/test_chat_cli_kv_cache.py OR extend existing CLI tests if any exist.

Minimum tests:
	1.	default uses kv-cache

	•	monkeypatch niels_gpt.generate.generate_ids_cached (and/or greedy cached) to record calls
	•	run the CLI entry function with args (no --no-kv-cache)
	•	assert cached generator was called

	2.	–no-kv-cache uses old path

	•	monkeypatch niels_gpt.generate.generate_text (or uncached ids function) to record calls
	•	run with --no-kv-cache
	•	assert uncached function was called instead

	3.	eot id is passed

	•	monkeypatch cached generator and assert it was called with eot_token_id == tokenizer.special_token_ids()["eot"] (or equivalent API)
	•	if tokenizer API differs, assert the resolved id equals the configured special id used elsewhere

If you do not currently have a clean CLI entrypoint for tests, you may:
	•	refactor main() into a run_chat_cli(argv: list[str]) -> int function with minimal changes, and test that.

⸻

done means
	•	running python -m niels_gpt.chat_cli (or your normal invocation) uses kv-cache by default
	•	--no-kv-cache reverts to the old behavior
	•	stopping is governed solely by <|eot|> token id
	•	tests pass
