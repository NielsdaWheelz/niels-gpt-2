pr-04 spec: sft formatting, masking, packing

goal

add a deterministic token-native chat format + assistant-only loss masking + packing/truncation to produce fixed-size training batches for sft, using the EOT sentinel as the only stop boundary.

non-goals
	•	do not modify tokenizer training, vocab, or special tokens.
	•	do not change dataset loaders (dolly/oasst/fineweb/wiki/gutenberg) except minimal adapters if strictly required.
	•	do not add new special tokens (no <|pad|>, no <|doc|>).
	•	do not touch model code, training loop, or generation (this PR only defines formatting + batch tensors + tests).

locked decisions

chat template (token-native)

each message becomes:
	•	system:
	•	<|ngpt_sys_84a5023f67d74cf29cc4001becde983c|> {system_text} <|ngpt_eot_84a5023f67d74cf29cc4001becde983c|>
	•	user:
	•	<|ngpt_usr_84a5023f67d74cf29cc4001becde983c|> {user_text} <|ngpt_eot_84a5023f67d74cf29cc4001becde983c|}
	•	assistant:
	•	<|ngpt_asst_84a5023f67d74cf29cc4001becde983c|> {assistant_text} <|ngpt_eot_84a5023f67d74cf29cc4001becde983c|>

conversations serialize as:
SYS once, then alternating USR/ASST turns.

system message policy: always include a system message. if absent, use:
	•	DEFAULT_SYSTEM_TEXT = "you are a helpful assistant."

stop condition

EOT token id only. no substring hacks here.

loss masking (assistant-only)

compute loss only on:
	•	assistant content tokens
	•	plus the assistant-ending EOT token

do not compute loss on:
	•	<|ngpt_sys_84a5023f67d74cf29cc4001becde983c|>, <|ngpt_usr_84a5023f67d74cf29cc4001becde983c|>, <|ngpt_asst_84a5023f67d74cf29cc4001becde983c|> tokens
	•	system content
	•	user content
	•	padding

truncation / packing

target sequence length for batching is S = T + 1 tokens, so we can create x and y of length T.

rules:
	1.	if serialized token ids length > S, left-truncate by dropping whole oldest turns (drop <|ngpt_usr_84a5023f67d74cf29cc4001becde983c|>...<|ngpt_eot_84a5023f67d74cf29cc4001becde983c|> + <|ngpt_asst_84a5023f67d74cf29cc4001becde983c|>...<|ngpt_eot_84a5023f67d74cf29cc4001becde983c|> pairs) until it fits.
	2.	never drop the final assistant turn.
	3.	if still too long after dropping whole turns (e.g. one huge assistant message), hard left-truncate within the remaining sequence to fit S, but ensure the last assistant EOT remains present (so the model can learn to stop).

padding

pad sequences shorter than S using pad_id = eot_id.
padding positions must have loss mask = 0.

targets and ignore index

produce:
	•	x: (B, T) int64
	•	y: (B, T) int64, with masked positions set to IGNORE_INDEX = -100
	•	loss_mask: (B, T) bool (optional but helpful)

mask alignment: loss is computed on y positions. if ids length is S, then y[i] corresponds to token ids[i+1]. therefore the y-loss-mask is mask_ids[1:].

⸻

inputs/outputs

canonical example type

pr-04 accepts a normalized chat example:

ChatMsg = TypedDict("ChatMsg", {"role": Literal["system","user","assistant"], "content": str})
ChatExample = TypedDict("ChatExample", {"messages": list[ChatMsg]})

invariant:
	•	messages may omit system (we inject default)
	•	messages may contain multiple user/assistant turns
	•	roles are lowercase

required functions (public API)

formatting

def serialize_chat_to_ids(
    ex: ChatExample,
    *,
    tokenizer,
    default_system_text: str,
) -> list[int]:
    """
    returns token ids for the whole chat using token-native template.
    must include <|ngpt_sys_84a5023f67d74cf29cc4001becde983c|> ... <|ngpt_eot_84a5023f67d74cf29cc4001becde983c|> first.
    """

masking

def sft_loss_mask_for_ids(
    ids: list[int],
    *,
    sys_id: int,
    usr_id: int,
    asst_id: int,
    eot_id: int,
) -> list[bool]:
    """
    returns mask per token-position in ids (same length).
    mask True only for assistant content tokens + assistant-ending eot.
    """

truncate + pad to S

def pack_sft_ids_and_mask(
    ids: list[int],
    mask: list[bool],
    *,
    S: int,          # S = T+1
    sys_id: int,
    usr_id: int,
    asst_id: int,
    eot_id: int,
    pad_id: int,
) -> tuple[list[int], list[bool]]:
    """
    enforces length exactly S:
      - left-truncate by whole turns where possible (drop oldest usr+asst pairs)
      - if still too long, hard left-truncate
      - right-pad with pad_id if too short
    returns (ids_packed, mask_packed)
    """

collate into tensors

def collate_sft_batch(
    packed: list[tuple[list[int], list[bool]]],
    *,
    T: int,
    device: str,
    ignore_index: int = -100,
) -> tuple[torch.LongTensor, torch.LongTensor, torch.BoolTensor]:
    """
    packed items have length S=T+1.
    returns:
      x: (B,T)
      y: (B,T) with ignore_index where loss_mask is False
      loss_mask: (B,T)
    """


⸻

algorithm constraints (so claude can’t invent nonsense)

parsing turns for truncation

turn boundaries are defined by role tokens and EOT.
	•	a “turn” is: [ROLE_TOKEN] ... [EOT]
	•	system turn is always first.
	•	user/assistant turns alternate (but be robust if input is imperfect).

dropping whole turns must remove complete segments from the left after the system segment.

assistant-only mask scanning rule

the mask must be computed by scanning ids:
	•	when you see asst_id, enter “assistant span” mode
	•	set mask False at the asst_id token itself
	•	set mask True for subsequent tokens until you see eot_id
	•	include that eot_id in mask True (assistant stop)
	•	after that, exit assistant span mode

mask must be False everywhere else.

⸻

files allowed to change
	•	add:
	•	format/sft.py (or your existing format module path)
	•	format/__init__.py if needed
	•	add tests:
	•	tests/test_sft_format.py
	•	optional minimal glue:
	•	if you have a “cache dataset” class for sft, you may add a small adapter in cache/dataset.py to return (ids,mask) items, but do not redesign the cache.

explicitly out of scope: training loop changes.

⸻

tests (must be explicit and golden)

create a tiny fake tokenizer in tests that:
	•	maps special tokens to fixed ids
	•	tokenizes normal text by splitting on spaces into deterministic fake ids

tests must verify:
	1.	serialization shape

	•	given a chat with no system message, serialization starts with <|ngpt_sys_84a5023f67d74cf29cc4001becde983c|> and ends with <|ngpt_eot_84a5023f67d74cf29cc4001becde983c|> for each turn.

	2.	mask correctness

	•	in a 1-turn chat, mask True only on assistant content tokens and the assistant EOT.

	3.	truncate by whole turns

	•	given a long multi-turn chat, packing drops oldest user+assistant pairs first, keeps last assistant turn.

	4.	pad behavior

	•	sequences shorter than S padded with pad_id, mask False on padding.

	5.	collate alignment

	•	x and y are correctly shifted
	•	y positions where loss_mask False are set to -100

⸻

acceptance criteria
	•	unit tests pass
	•	formatting + packing is deterministic
	•	no substring stop logic introduced anywhere in these modules
	•	collate_sft_batch returns correctly shaped tensors and correct ignore-index masking
