pr-04 spec: streams + batching (+ disk cache)

goal

implement:
	1.	streams.py: build cached bytes streams for each source/split (wiki, roam, primer) and return sources_train, sources_val.
	2.	batching.py: sample mixed-source batches deterministically (given a torch.Generator) according to p_train.

must match interfaces.md.

non-goals
	•	no model code
	•	no training loop changes
	•	no refactors of pr-01..03 beyond import-path fixes required to integrate
	•	no “data cleaning” beyond what loaders already do (wikitext drops empty lines; keep that)
	•	no fancy streaming / mmap; simple .bin cache files are enough

repo assumptions (from existing code)
	•	package root is the existing python package directory in this repo (do not create a second one if capitalization differs).
	•	config.py exists with TrainConfig, ModelConfig, and default_p_train() returning {"wiki":0.784, "roam":0.196, "primer":0.020}.
	•	data/load_wikitext() exists and returns dict keys train|val|test of list[str] (empty-only lines already removed).
	•	roam markdown root: .roam-data/
	•	primer path: data/primer.txt

decisions locked
	•	encoding: utf-8
	•	document separator (wiki/roam): exactly "\n\n" between documents
	•	primer: take raw text as-is (no added separators), then utf-8 encode
	•	splits:
	•	wiki: use wikitext train and val splits from loader
	•	roam: split by file paths (held-out files)
	•	primer: split by dialogue blocks on delimiter \n\n<dialogue>\n\n (held-out dialogues)
	•	errors: fail fast if any requested source missing/empty or shorter than T+1
	•	caching: cache .bin per (source, split) in data/cache/streams/ with .meta.json for invalidation
	•	determinism: get_batch(..., generator=g) uses only torch RNG + that generator
	•	source gating: build only enabled_sources; required_sources must be subset of enabled_sources. defaults: enabled_sources=("wiki","roam","primer"), required_sources=("wiki","roam","primer")

files

create:
	•	streams.py
	•	batching.py
	•	tests/test_streams.py
	•	tests/test_batching.py

(add directories if missing: data/cache/streams/)

interfaces

streams.py

from __future__ import annotations
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Tuple

BytesSources = Dict[str, bytes]  # keys: "wiki" | "roam" | "primer"

@dataclass(frozen=True)
class StreamBuildConfig:
    roam_root: str = ".roam-data/"
    primer_path: str = "data/primer.txt"
    cache_dir: str = "data/cache/streams"
    doc_separator: str = "\n\n"
    roam_val_frac: float = 0.10
    primer_val_frac: float = 0.10
    seed: int = 42
    delimiter: str = "\n\n<dialogue>\n\n"
    force_rebuild: bool = False


def build_sources(cfg: StreamBuildConfig) -> Tuple[BytesSources, BytesSources]:
    """
    returns (sources_train, sources_val)

    sources_* keys are subset of {"wiki","roam","primer"}.
    each value is a bytes stream encoded utf-8.

    caching:
      - if cache exists and metadata matches current inputs, load .bin
      - else rebuild, write .bin and .meta.json
    """

support functions (must exist; exact names required):

def build_wiki_stream(docs: list[str], *, sep: str) -> bytes:
    """utf-8 encode docs and join with sep."""

def build_roam_stream(docs: list[str], *, sep: str) -> bytes:
    """utf-8 encode docs and join with sep."""

def build_primer_stream(text: str) -> bytes:
    """utf-8 encode text exactly as-is."""

metadata + invalidation contract:
	•	for wiki:
	•	store: {"source":"wiki","split":..., "n_docs":..., "total_chars":..., "sep":...}
	•	(wikitext content is stable; this is “good enough”)
	•	for roam:
	•	store: list of {"path": str, "mtime_ns": int, "size": int} for the paths included in that split, plus sep
	•	for primer:
	•	store: {"path":..., "mtime_ns":..., "size":..., "delimiter":...} plus split name

if any metadata mismatches, rebuild.

cache layout:
	•	{cache_dir}/{source}_{split}.bin
	•	{cache_dir}/{source}_{split}.meta.json

splitting contract (calls your existing data modules; adjust import paths to match repo):
	•	roam:
	•	paths = list_roam_paths(cfg.roam_root)
	•	(train_paths, val_paths) = split_roam_paths(paths, val_frac=cfg.roam_val_frac, seed=cfg.seed)
	•	train_docs = load_texts(train_paths) etc.
	•	primer:
	•	text = load_primer_text(cfg.primer_path)
	•	(train_text, val_text) = split_primer_dialogues(text, val_frac=cfg.primer_val_frac, seed=cfg.seed, delimiter=cfg.delimiter)
	•	wiki:
	•	wiki = load_wikitext()
	•	wiki_train_docs = wiki["train"]
	•	wiki_val_docs = wiki["val"]

length guard:
	•	after building bytes stream, if len(stream) < cfg.T + 1 you must raise a ValueError listing offending sources/splits and lengths.
	•	note: cfg doesn’t contain T; you can either:
	•	import ModelConfig and use its default T=256, OR
	•	accept T as an optional arg to build_sources.
	•	choose one and be consistent; simplest: from config import ModelConfig; T = ModelConfig().T

batching.py

from __future__ import annotations
from typing import Dict, Tuple
import torch

BytesSources = Dict[str, bytes]

def get_batch(
    sources: BytesSources,
    *,
    p: Dict[str, float],
    B: int,
    T: int,
    device: str,
    generator: torch.Generator | None = None,
) -> Tuple[torch.LongTensor, torch.LongTensor]:
    """
    sources: dict source_name -> bytes stream
    p: dict source_name -> prob; must sum to 1.0 (within 1e-6) and keys must match sources
    behavior:
      - sample source choice for each of B items via torch.multinomial(probs, B, replacement=True, generator=generator)
      - for each chosen source:
          - sample i via torch.randint(0, len(src)-(T+1)+1, (1,), generator=generator)
          - chunk = src[i:i+T+1]
          - x = chunk[:-1], y = chunk[1:]
      - return x,y as (B,T) int64 on device
    invariants:
      - values in [0..255]
      - y[:, :-1] == x[:, 1:]
    errors:
      - if any p key missing from sources or any source missing from p: raise
      - if abs(sum(p)-1) > 1e-6: raise
      - if any source len < T+1: raise listing sources and lengths
    determinism:
      - if generator is provided, use it for multinomial + randint; no python random.
    """

implementation notes:
	•	converting bytes chunk to tensor: use torch.tensor(list(chunk), dtype=torch.int64, device=device) (simple, fine at B=32).
	•	keep code simple; vectorization optional.

tests

tests/test_streams.py

use tmp_path as cache dir; do not touch real .roam-data/.
	•	create fake roam md files in temp dir; create a tiny primer file; monkeypatch config paths via StreamBuildConfig(roam_root=..., primer_path=..., cache_dir=..., seed=42)
	•	verify:
	•	build_sources returns dicts with keys wiki/roam/primer (if wiki loader is hard to use in tests, allow build_sources to accept injected docs; if you do that, expose a second function build_sources_from_docs(...) and test that. keep public API build_sources(cfg) intact.)
	•	caches are created (.bin and .meta.json)
	•	second call with same inputs loads from cache (can assert by modifying a file and checking rebuild happens; or by checking mtime of cache file doesn’t change)

tests/test_batching.py
	•	build tiny sources:
	•	sources={"wiki": b"abcdefghijklmnopqrstuvwxyz"*50, "roam": b"0123456789"*200, "primer": b"system: x\nuser: y\nassistant: z\n"*50}
	•	set T=16, B=8
	•	check:
	•	shapes (B,T)
	•	dtype int64
	•	range 0..255
	•	shift invariant
	•	determinism:
	•	g1=torch.Generator().manual_seed(0)
	•	g2=torch.Generator().manual_seed(0)
	•	x1,y1=get_batch(..., generator=g1) and x2,y2=get_batch(..., generator=g2) are identical
	•	error fast:
	•	bad probs sum ≠1 raises
	•	missing key raises
	•	too-short source raises listing offending sources

acceptance / done when
	•	pytest -q passes
	•	python -c "from streams import build_sources, StreamBuildConfig; print(build_sources(StreamBuildConfig())[0].keys())" runs (even if wiki download is slow, it should be correct)
	•	cache files appear under data/cache/streams/ after running build once


determinism contract (torch.Generator)

you want reproducible batches. that means: given the same seed, you must make every random choice using one torch RNG.
