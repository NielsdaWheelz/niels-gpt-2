pr-06 spec

name: pr-06: training loop + lr schedule + checkpoints + eval

goal
implement a reproducible training entrypoint that:
	•	builds train/val byte sources via niels_gpt.streams.build_sources() (sole data provider)
	•	trains GPT(ModelConfig) with adamw + warmup+cosine lr
	•	periodically evaluates val_wiki loss
	•	saves latest.pt, periodic step_*.pt, and best.pt (best by lowest val_wiki loss)
	•	supports a 1000-step smoke run and a 20k-step full run via config files

non-goals
	•	mixed precision
	•	distributed training
	•	fancy logging frameworks
	•	changing model architecture / batching logic / stream cache format beyond adding a thin wrapper if needed

⸻

allowed changes

add
	•	niels_gpt/train.py
	•	niels_gpt/lr_schedule.py
	•	niels_gpt/checkpoint.py
	•	niels_gpt/eval.py
	•	configs/smoke.json
	•	configs/train.json
	•	tests/test_lr_schedule.py
	•	tests/test_checkpoint_roundtrip.py

modify
	•	niels_gpt/config.py (only to add fields / config loading helpers; do not remove existing fields)
	•	niels_gpt/streams.py (only if necessary to add a wrapper that returns (train_sources, val_sources) without breaking existing behavior)

do not modify
	•	niels_gpt/model/gpt.py and model internals (except imports if needed)
	•	get_batch implementation
	•	tokenizer/chat formatting modules

⸻

hard dependencies (must use)
	•	model:
	•	from niels_gpt.model.gpt import GPT
	•	model = GPT(model_cfg) where model_cfg: ModelConfig is a frozen dataclass
	•	weight tying already exists (don’t re-implement)
	•	device:
	•	from niels_gpt.device import get_device → returns "mps" or "cpu"
	•	data:
	•	niels_gpt.streams.build_sources() is the only way training obtains byte streams
	•	batching:
	•	get_batch(sources, *, p, B, T, device, generator=None)
	•	paths:
	•	configs dir: CONFIG_DIR from niels_gpt.paths
	•	checkpoint dir: CHECKPOINT_DIR from niels_gpt.paths

⸻

config contract

configs are json files at configs/.

schema (top-level json):

{
  "model": { /* overrides for ModelConfig */ },
  "train": { /* overrides for TrainConfig */ }
}

override rules
	•	unknown keys in either section → raise ValueError listing the unknown keys
	•	missing keys → use dataclass defaults
	•	do not mutate dataclasses (use dataclasses.replace)

TrainConfig additions required
extend TrainConfig (keep existing fields) with:
	•	eval_steps: int  (default 100)
	•	log_every: int   (default 50)
	•	p_train: dict[str, float] (ensure non-None by default; see below)
	•	optionally p_val: dict[str, float] | None (not required; can hardcode val to wiki-only)

probabilities
	•	training probs must be:
	•	primer weight 2% overall
	•	wiki/roam split 80/20 of the remaining 98%
	•	therefore:
	•	wiki = 0.784
	•	roam = 0.196
	•	primer = 0.020
	•	implement this as the default TrainConfig.p_train if none provided.

⸻

lr schedule contract

create niels_gpt/lr_schedule.py:

def lr_at_step(
    step: int,
    total_steps: int,
    *,
    base_lr: float,
    warmup_steps: int,
    min_lr: float,
) -> float:
    """
    - if step < warmup_steps: linear warmup from 0 -> base_lr
    - else: cosine decay from base_lr -> min_lr over remaining steps
    - clamp to [min_lr, base_lr]
    - step is in [0, total_steps-1]
    """


⸻

checkpoint contract

create niels_gpt/checkpoint.py:

def save_checkpoint(
    path: str,
    *,
    model_cfg: dict,
    train_cfg: dict,
    model: GPT,
    optimizer: torch.optim.Optimizer | None,
    step: int,
    best_val_loss: float | None,
) -> None:
    """torch.save dict with cfgs + model_state + optimizer_state (if optimizer not None) + step + best_val_loss"""

def load_checkpoint(path: str, *, device: str) -> dict:
    """
    returns the saved dict with:
      - model_cfg, train_cfg
      - model_state
      - optimizer_state (optional)
      - step
      - best_val_loss (optional)
    """

resume compatibility
	•	when resuming, enforce that model-shape fields match current config:
	•	V,T,C,L,H,d_ff,dropout,rope_theta
	•	mismatch → raise ValueError with a clear diff.

checkpoint files
in CHECKPOINT_DIR:
	•	periodic: step_{step:07d}.pt (save every ckpt_every)
	•	latest: latest.pt (overwrite every checkpoint save)
	•	best: best.pt (overwrite only when val_wiki_loss improves)

⸻

evaluation contract

create niels_gpt/eval.py:

@torch.no_grad()
def eval_loss_on_stream(
    model: GPT,
    *,
    stream: bytes,
    B: int,
    T: int,
    device: str,
    eval_steps: int,
    seed: int,
) -> float:
    """
    deterministic eval:
      - create a local torch.Generator(device="cpu") seeded with seed
      - pass it into get_batch(... generator=gen)
    computes mean CE loss over eval_steps batches from this one stream.
    """

training-time eval uses val wiki stream only.

⸻

training entrypoint contract

create niels_gpt/train.py runnable as:

python -m niels_gpt.train --config configs/smoke.json
python -m niels_gpt.train --config configs/train.json
python -m niels_gpt.train --config configs/train.json --resume checkpoints/latest.pt

args
	•	--config (required)
	•	--resume (optional)
	•	--device (optional; default from get_device())

behavior
	1.	load config json → ModelConfig, TrainConfig
	2.	select device
	3.	obtain sources via build_sources():
	•	must produce:
	•	train_sources: dict[str, bytes] including "wiki", "roam", "primer" (as available)
	•	val_sources: dict[str, bytes] including "wiki" at least
	•	if existing build_sources() doesn’t provide train vs val cleanly, add a wrapper in streams.py:
	•	def build_train_val_sources(model_cfg, train_cfg) -> tuple[dict[str, bytes], dict[str, bytes]]
	•	wrapper may read cached .bin outputs already generated by existing pipeline
	•	do not break current cache logic
	4.	init model + optimizer:
	•	optimizer: torch.optim.AdamW
	•	lr set each step via lr_at_step(...)
	•	weight decay: 0.1 (use train_cfg field if present; else constant)
	•	betas: (0.9, 0.95) (constant for v0)
	5.	training loop for total_steps:
	•	sample batch from train_sources with p_train
	•	forward → logits
	•	loss = cross entropy over all positions
	•	backward
	•	grad clip global norm = train_cfg.grad_clip (1.0)
	•	optimizer step
	•	every log_every steps: print step, loss, lr
	•	every eval_every steps:
	•	compute val_wiki_loss with eval_steps=train_cfg.eval_steps
	•	if improved: save best.pt
	•	every ckpt_every steps:
	•	save periodic step_*.pt
	•	save/overwrite latest.pt
	6.	always save a final latest.pt at end.

smoke vs full configs
	•	smoke: total_steps=1000, eval_steps=50
	•	full: total_steps=20000, eval_steps=100
	•	both: seed=42, B=32, T=256, eval_every=200, ckpt_every=1000

⸻

acceptance tests

tests/test_lr_schedule.py
	•	lr_at_step(0, total_steps, ...) == 0.0 (or extremely close)
	•	lr_at_step(warmup_steps, ...) == base_lr (within tiny tolerance)
	•	lr_at_step(total_steps-1, ...) approx min_lr (within tolerance)
	•	monotonic non-decreasing during warmup
	•	non-increasing after warmup

tests/test_checkpoint_roundtrip.py
	•	construct model+optimizer with fixed seed
	•	do one train step on a tiny synthetic batch (can create random x,y directly; do not depend on datasets)
	•	save checkpoint
	•	load checkpoint into fresh model+optimizer
	•	assert:
	•	parameters exactly equal (torch.testing.assert_close(..., rtol=0, atol=0) on cpu)
	•	step restored
	•	optimizer state restored (at least that state_dict() keys match)

⸻

configs to add

configs/smoke.json

{
  "model": {},
  "train": {
    "seed": 42,
    "B": 32,
    "total_steps": 1000,
    "eval_every": 200,
    "eval_steps": 50,
    "ckpt_every": 1000,
    "base_lr": 0.0003,
    "warmup_steps": 200,
    "min_lr": 0.00003,
    "grad_clip": 1.0,
    "p_train": {"wiki": 0.784, "roam": 0.196, "primer": 0.02},
    "log_every": 50
  }
}

configs/train.json

{
  "model": {},
  "train": {
    "seed": 42,
    "B": 32,
    "total_steps": 20000,
    "eval_every": 200,
    "eval_steps": 100,
    "ckpt_every": 1000,
    "base_lr": 0.0003,
    "warmup_steps": 200,
    "min_lr": 0.00003,
    "grad_clip": 1.0,
    "p_train": {"wiki": 0.784, "roam": 0.196, "primer": 0.02},
    "log_every": 50
  }
}
