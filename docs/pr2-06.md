pr-06 spec: two-phase training runner, eval, checkpoints

goal

implement a training runner that supports:
	•	phase=pretrain (train on stream caches; evaluate on wiki_val)
	•	phase=sft (train on sft example caches; evaluate on wiki_val by default, optionally sft_val if present)
	•	periodic eval + periodic checkpointing + best checkpoint selection
	•	resume from checkpoints/latest.pt or an explicit checkpoint path
	•	no changes to model code (pr-5) and no changes to dataset/cache formats (pr-1..5)

non-goals
	•	mixed precision, grad accumulation, activation checkpointing (pr-7)
	•	new datasets/loaders/cache builders (assumed already implemented in pr-1..5; pr-6 only consumes caches)
	•	any web/ui work

locked interfaces & invariants (must not change)

cache layout (pretrain)
	•	data/cache/streams/{source}_{split}.bin
	•	source ∈ {wiki, roam, primer}
	•	split ∈ {train, val}
	•	each has {same_path}.meta.json

cache layout (sft)
	•	data/cache/sft/{source}_{split}.bin
	•	source ∈ {dolly, oasst1, primer}
	•	split ∈ {train, val}
	•	each has {same_path}.meta.json

sft batching contract
	•	SFTExampleDataset.get_batch() returns (x, y, y_masked)
	•	y_masked uses -100 for positions that should be ignored by CE
	•	masking rule: after asst_id, compute loss until eot_id (exclusive or inclusive doesn’t matter as long as consistent; define it)

checkpoint format (must match exactly)

{
  "model_cfg": dict,
  "train_cfg": dict,
  "model_state": state_dict,
  "optimizer_state": state_dict | None,
  "step": int,
  "best_val_loss": float | None,
}

checkpoint directory + naming
	•	CHECKPOINT_DIR = REPO_ROOT / "checkpoints"
	•	write:
	•	checkpoints/step_{step:07d}.pt (every ckpt_every)
	•	checkpoints/latest.pt (overwrite every ckpt_every)
	•	checkpoints/best.pt (overwrite when best improves)
	•	pr-8 expects checkpoints/best.pt to exist and be loadable.

defaults (from TrainConfig)
	•	seed=42
	•	eval_every=200
	•	ckpt_every=1000
	•	B=32
	•	total_steps=20000
	•	base_lr=3e-4, warmup_steps=200, min_lr=3e-5
	•	grad_clip=1.0
	•	accum_steps=1 (present but unused beyond dividing loss if you already do that)

⸻

CLI contract (exact; do not invent flags)

add a single entrypoint module:
	•	python -m train.run --phase pretrain --config configs/pretrain.json
	•	python -m train.run --phase sft --config configs/sft.json
	•	python -m train.run --phase pipeline --config configs/pipeline.json

flags:
	•	--phase one of {pretrain, sft, pipeline}
	•	--config path to json
	•	--device optional {mps,cpu}; default auto
	•	--resume optional path; if omitted and checkpoints/latest.pt exists, auto-resume unless --no-resume
	•	--no-resume forces fresh start

pipeline semantics:
	•	run pretrain first, producing best.pt
	•	then start sft, initializing from pretrain best (load model weights; optimizer resets)

⸻

config files (json) required in this PR

configs/pretrain.json
	•	must include:
	•	model_cfg_path (path to saved model cfg json OR inline model_cfg dict; pick one and be consistent)
	•	train_cfg: includes all existing train knobs
	•	sources: list of pretrain sources to mix, with probs
	•	default: {"wiki": 0.784, "roam": 0.196, "primer": 0.020}
	•	val_source: default "wiki"

configs/sft.json
	•	must include:
	•	same model cfg reference
	•	train_cfg
	•	sft_sources: list/probs from {dolly, oasst1, primer} (even if only one exists initially)
	•	val_source: default "wiki"; if data/cache/sft/*_val.bin exist, allow "sft" for sft-val evaluation

configs/pipeline.json
	•	includes pretrain_config_path + sft_config_path

constraint: configs are treated as the source of truth and stored verbatim in checkpoints under "train_cfg" (and model cfg under "model_cfg").

⸻

training loop behavior

for step in [start_step .. total_steps):
	•	sample batch
	•	forward
	•	compute loss
	•	pretrain: CE over all tokens
	•	sft: CE using y_masked ignore_index = -100
	•	backward
	•	clip grads
	•	optimizer step
	•	update lr schedule
	•	every eval_every: run eval on val dataset for eval_batches batches (configurable; default 50)
	•	every ckpt_every: save step_*.pt and latest.pt, and if eval just ran and improved, update best.pt

logging
	•	print progress every 50 steps:
	•	step, lr, train_loss (moving avg), last_val_loss (if available), tokens/sec (optional)

resume
	•	if resuming, restore:
	•	model_state
	•	optimizer_state (if present)
	•	step
	•	best_val_loss
	•	verify loaded model_cfg matches current config on shape-critical keys (V,T,C,L,H,d_ff at minimum)

⸻

failure behavior (must be explicit)
	•	if required cache files are missing:
	•	show exact missing paths
	•	show expected naming convention
	•	exit with nonzero code
	•	if sft caches missing and phase=sft:
	•	same (no silent fallback to pretrain)

⸻

acceptance tests (must be added in this PR)

add tests/test_train_runner_smoke.py with three tests:
	1.	pretrain smoke (cpu)

	•	create tiny fake cache files in a temp dir (or use a small in-repo fixture if you already have one)
	•	run run_pretrain(...) for 5–10 steps
	•	assert:
	•	loss is finite
	•	latest.pt exists
	•	step_000000X.pt exists

	2.	resume smoke

	•	run 5 steps → save latest
	•	resume → run 5 more
	•	assert final step == 10 and checkpoint step increments correctly

	3.	sft masking smoke

	•	build a tiny fake sft batch containing <|asst|> and <|eot|> ids
	•	ensure loss ignores masked positions (e.g., compare to manual computation on unmasked positions)

(keep tests cpu-only, tiny tensors.)
