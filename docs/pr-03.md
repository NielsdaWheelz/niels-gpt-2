pr-03 spec: datasets + splits (offline tests)

goal

add loaders and deterministic splits for:
	•	wikitext (via datasets)
	•	roam markdown corpus (paths under .roam-data/)
	•	primer dialogues (data/primer.txt)

all logic must follow interfaces.md and produce stable outputs under seed=42.

constraints
	•	python 3.11
	•	pytest, run via: pytest tests
	•	package layout is correct; do not restructure repo
	•	tests must be fully offline (no dataset downloads)
	•	allowed deps: stdlib + datasets (for real loader code only). tests must mock it.

files allowed to change

create/modify only:
	•	niels_gpt/data/__init__.py (new)
	•	niels_gpt/data/wikitext.py (new)
	•	niels_gpt/data/roam.py (new)
	•	niels_gpt/data/primer.py (new)
	•	tests/test_data_loading.py (new)

do not modify existing files (config/device/rng/chat/tokenizer/tests) in this pr.

⸻

public api (exact signatures)

niels_gpt/data/wikitext.py

from typing import Dict, List

def load_wikitext() -> dict[str, list[str]]:
    """
    loads via datasets.load_dataset("wikitext", "wikitext-103-raw-v1")
    returns keys: "train", "val", "test"
    maps hf "validation" -> "val"
    drops empty-only lines: keep line iff line.strip() != ""
    """

niels_gpt/data/roam.py

from typing import List, Tuple

def list_roam_paths(root_dir: str) -> list[str]:
    """
    recursively list *.md under root_dir
    return absolute paths (as strings), sorted deterministically
    """

def load_texts(paths: list[str]) -> list[str]:
    """
    read each file as utf-8 with errors="replace"
    returns list[str] same length/order
    """

def split_roam_paths(
    paths: list[str],
    *,
    val_frac: float = 0.1,
    seed: int,
) -> tuple[list[str], list[str]]:
    """
    deterministic split by file path, not bytes
    uses seed for shuffle
    returns (train_paths, val_paths)

    rules:
    - if len(paths) < 2: val_paths == []
    - else val_size = max(1, int(len(paths) * val_frac))
    - ensure train and val are disjoint and cover all paths
    """

niels_gpt/data/primer.py

from typing import Tuple

DIALOGUE_DELIM = "\n\n<dialogue>\n\n"

def load_primer_text(path: str) -> str:
    """returns raw text exactly as stored (utf-8, errors='replace')."""

def split_primer_dialogues(
    text: str,
    *,
    val_frac: float = 0.1,
    seed: int,
) -> tuple[str, str]:
    """
    split by dialogue blocks (not bytes).
    delimiter: DIALOGUE_DELIM

    parse rules:
    - blocks = text.split(DIALOGUE_DELIM)
    - for each block: block_stripped = block.strip()
    - drop empty blocks (block_stripped == "")
    - split blocks deterministically using seed

    split rules:
    - if num_blocks < 2: val_text == ""
    - else val_size = max(1, int(num_blocks * val_frac))
    - return (train_text, val_text) as DIALOGUE_DELIM-joined blocks (no leading/trailing delimiter)
    """

niels_gpt/data/__init__.py

export the public API:

from .wikitext import load_wikitext
from .roam import list_roam_paths, load_texts, split_roam_paths
from .primer import load_primer_text, split_primer_dialogues, DIALOGUE_DELIM


⸻

test plan (acceptance)

add tests/test_data_loading.py with offline tests only.

wikitext loader (offline)
	•	monkeypatch niels_gpt.data.wikitext.load_dataset to return a fake object that mimics:
	•	ds["train"]["text"], ds["validation"]["text"], ds["test"]["text"]
	•	ensure load_wikitext() returns keys train/val/test
	•	ensure empty-only strings are dropped (e.g. "   " removed)
	•	ensure non-empty strings preserved

roam

use tmp_path:
	•	create nested dirs with a.md, b.md, sub/c.md plus a non-md file
	•	list_roam_paths(tmp_path) returns absolute sorted paths and only .md
	•	split_roam_paths(paths, seed=42) is deterministic across calls
	•	load_texts returns utf-8 strings (use a file with invalid bytes; ensure it doesn’t crash due to errors="replace")

primer

use a tiny in-memory string:
	•	include multiple blocks separated by DIALOGUE_DELIM, with:
	•	leading/trailing delimiter
	•	an empty block in the middle
	•	split_primer_dialogues(..., seed=42) drops empty blocks
	•	deterministic split across calls
	•	train/val outputs when re-split are identical (given same seed)
	•	if only 1 block, val is ""

⸻

definition of done
	•	pytest tests passes locally without internet
	•	importing works:
	•	from niels_gpt.data import load_wikitext, list_roam_paths, split_primer_dialogues

⸻

implementation notes

dependencies
	•	added "datasets" to pyproject.toml dependencies (required for wikitext loader)
	•	tests use monkeypatching to avoid actual dataset downloads

completed
	✅ all 5 files created/implemented:
		- niels_gpt/data/__init__.py
		- niels_gpt/data/wikitext.py
		- niels_gpt/data/roam.py
		- niels_gpt/data/primer.py
		- tests/test_data_loading.py
	✅ 13 tests passing (all offline)
	✅ public API imports working
	✅ deterministic splits using stdlib random.Random(seed)
	✅ pathlib.Path for path operations
	✅ utf-8 with errors="replace" for file reading
	✅ full test suite passing (30/30 tests)
