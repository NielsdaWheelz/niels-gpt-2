# PR-07: generation + cli chat

## goal
add reusable text generation utilities (temperature + top-k, rope-safe cropping) and a minimal interactive cli chat that loads a trained checkpoint and responds in an “assistant” transcript format.

## non-goals
- kv-cache / fast generation
- fancy tui, multiline editing, streaming tokens
- changing training/model/checkpoint formats
- adding new dependencies beyond existing ones

## repo interfaces (MUST USE; do not reimplement)
- checkpoint loader:
  - module: `niels_gpt/checkpoint.py`
  - `load_checkpoint(path: str, *, device: str) -> dict` returns keys:
    - `model_cfg: dict`, `train_cfg: dict`, `model_state`, `optimizer_state|None`, `step`, `best_val_loss|None`
- model config:
  - `niels_gpt/config.py` dataclass `ModelConfig` (includes `T`)
- model:
  - `niels_gpt/model/gpt.py` class `GPT(cfg: ModelConfig)`
- tokenizer:
  - `niels_gpt/tokenizer.py` `encode(text)->LongTensor` (cpu, 1d), `decode(ids)->str`
- chat formatting/parsing:
  - `niels_gpt/chat_format.py` `format_chat(messages)->str` (ends with `"assistant: "`), `extract_assistant_reply(generated_text)->str`
- device:
  - `niels_gpt/device.py` `get_device()->"mps"|"cpu"`
- test runner: `pytest`

## files allowed to change / add
ADD:
- `niels_gpt/generate.py`
- `niels_gpt/chat_cli.py`
- `tests/test_generate.py`
- `tests/test_chat_cli_stop.py`

DO NOT MODIFY:
- anything under `niels_gpt/model/`
- tokenizer/chat_format/checkpoint/config/device modules
- training code

## public api to implement

### `niels_gpt/generate.py`

#### 1) `top_k_filter`
```py
def top_k_filter(logits: torch.Tensor, k: int) -> torch.Tensor:
    """
    logits: (..., V)
    returns: same shape, where all but top-k entries are set to -inf.
    k must satisfy 1 <= k <= V.
    """

2) sample_next_token

def sample_next_token(
    logits_1d: torch.Tensor,
    *,
    temperature: float,
    top_k: int | None,
    generator: torch.Generator | None,
) -> int:
    """
    logits_1d: (V,) float tensor (on any device)
    behavior:
      - if temperature == 0: return argmax (deterministic)
      - else:
          scaled = logits_1d / temperature
          if top_k is not None: apply top_k_filter(scaled, top_k)
          probs = softmax(scaled)
          sample 1 index via torch.multinomial(probs, 1, generator=generator)
    returns: python int in [0..V-1]
    """

3) generate_ids

def generate_ids(
    model: torch.nn.Module,
    prompt_ids: torch.LongTensor,
    *,
    max_new_tokens: int,
    T: int,
    temperature: float = 0.9,
    top_k: int | None = 50,
    stop_sequences: list[bytes] | None = None,
    device: str,
    generator: torch.Generator | None = None,
) -> torch.LongTensor:
    """
    prompt_ids: (n,) int64 on cpu (as returned by tokenizer.encode)
    returns: (n + m,) int64 on cpu, where m <= max_new_tokens

    rules:
    - model runs on `device`; ids passed to model are moved to `device`.
    - before each forward pass, crop context to last T tokens:
        ctx = ids[-T:] if len(ids) > T else ids
      (must satisfy GPT assertion T_cur <= cfg.T)
    - forward: logits = model(ctx[None, :]) -> (1, t, V)
      take last position logits: logits[0, -1] -> (V,)
    - sample next token via sample_next_token(...)
    - append to ids (maintained on cpu for stop checks and decode compatibility)

    stopping:
    - if stop_sequences is provided, stop generation when ANY stop sequence appears
      in the newly generated portion, aligned on byte boundaries.
    - required stop sequences for chat: b"\\nuser: " and b"\\nsystem: "
    - when a stop sequence is detected, truncate output to just before the stop sequence
      begins (do not include the stop tag bytes in the returned ids).

    safety:
    - use torch.no_grad()
    - set model.eval() for generation
    """

4) generate_text

def generate_text(
    model: torch.nn.Module,
    prompt_text: str,
    *,
    cfg: "ModelConfig",
    max_new_tokens: int,
    temperature: float = 0.9,
    top_k: int | None = 50,
    stop_sequences: list[bytes] | None = None,
    device: str,
    generator: torch.Generator | None = None,
) -> str:
    """
    - prompt_text -> prompt_ids via tokenizer.encode
    - generate_ids(..., T=cfg.T, ...)
    - decode full ids via tokenizer.decode
    - return decoded text
    """

Stop sequence detection implementation constraint:
	•	must be correct and simple. allowed approach:
	•	keep start = len(prompt_ids)
	•	after each token append, search within a bounded tail window of the generated bytes:
	•	let max_stop = max(len(s) for s in stop_sequences)
	•	inspect ids_bytes[max(0, len(ids)-max_stop-4):] (any small extra slack is fine)
	•	if a stop seq occurs at an index >= start, stop and truncate.

niels_gpt/chat_cli.py

entrypoint:
	•	must be runnable as: python -m niels_gpt.chat_cli

cli arguments (argparse):
	•	--ckpt PATH (required): checkpoint path
	•	--max-new-tokens N (default 256)
	•	--temperature X (default 0.9)
	•	--top-k K (default 50; allow 0 or none to mean no top-k)
	•	--seed S (default 42) used only for generation sampling determinism
	•	--system TEXT (optional) system message content
	•	--system-file PATH (optional) read system message from file (utf-8)
	•	precedence: --system-file > --system > default system prompt

default system prompt (exact string):
	•	you are a tiny chatbot on niels’ website. answer in third person about niels. do not invent personal facts; if you don’t know, say you don’t know.

interaction loop behavior:
	•	maintain messages: list[dict] in {role, content} format
	•	initialize with one system message
	•	repeatedly:
	1.	read one line from stdin (input("> "))
	2.	if line is empty, continue
	3.	if line == “/exit”: exit cleanly
	4.	append user message
	5.	prompt = format_chat(messages) (must end with "assistant: ")
	6.	call generate_text with stop_sequences=[b"\nuser: ", b"\nsystem: "]
	7.	reply = extract_assistant_reply(generated_text)
	8.	print reply (just the reply, not the whole transcript)
	9.	append assistant message with that reply
	•	ctrl-d / EOF should exit cleanly (catch EOFError)

model loading:
	•	device = get_device()
	•	ckpt = load_checkpoint(args.ckpt, device=device)
	•	cfg = ModelConfig(**ckpt["model_cfg"])
	•	model = GPT(cfg); model.load_state_dict(ckpt["model_state"]); model.to(device)
	•	model.eval()

acceptance tests (MUST ADD)

tests must not require a real checkpoint file.

tests/test_generate.py
	1.	temperature=0 returns argmax:

	•	make logits where argmax is known; ensure sample_next_token(... temperature=0 ...) equals argmax.

	2.	top_k=1 matches argmax even when temperature>0:

	•	same logits; ensure top_k=1 always returns argmax for repeated calls.

	3.	generator determinism:

	•	with same logits, temperature>0, top_k=None, and two generators seeded the same:
	•	sampled token sequence over N draws must match.

	4.	top_k_filter masks correctly:

	•	verify exactly k finite values remain (others are -inf).

tests/test_chat_cli_stop.py

stop-sequence truncation correctness in generate_ids:
	•	create a tiny dummy model that emits a forced sequence of next tokens corresponding to:
	•	b"hello\nuser: " after the prompt.
	•	call generate_ids(... stop_sequences=[b"\nuser: "], max_new_tokens large enough ...)
	•	assert returned decoded text includes "hello" but DOES NOT include "\nuser: ".

(implement dummy model as a small nn.Module that returns logits with a single very-large value at the forced next byte.)

done when
	•	pytest passes
	•	python -m niels_gpt.chat_cli --ckpt <path> starts an interactive chat and responds.
