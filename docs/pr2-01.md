# pr-01: sentencepiece tokenizer + token-native chat template

## goal

replace the byte-level tokenizer with a subword tokenizer and make chat structure + stopping token-native.

at the end of this PR we can:
- train a sentencepiece tokenizer from local text files (roam + primer + optional extra globs)
- encode/decode text to token ids
- format chats into token sequences using special tokens
- extract the assistant reply from generated token ids
- assert all invariants via unit tests

## locked decisions

- tokenizer library: **sentencepiece**
- algorithm: **unigram**
- vocab size: **16000**
- special tokens (exactly these 4, no more):  
  `<|sys|>`, `<|usr|>`, `<|asst|>`, `<|eot|>`
- stop condition (future generation): **stop only on `<|eot|>` token id**
- tokenizer training text: local files only in this PR (file globs); hf datasets come later
- normalization: convert `\r\n` to `\n` when reading files
- sentencepiece config:
  - `byte_fallback = true` (recommended robustness)
  - `character_coverage = 1.0`

## non-goals

- no dataset loaders (fineweb/wikitext/dolly/oasst) in this PR
- no training loop changes
- no generation changes (we only provide template + stop token id)
- no `<|pad|>` token
- no fancy chat masking logic; just formatting + extraction utilities

## new dependency

- add `sentencepiece` as a dependency:
  - if `pyproject.toml` exists, add it there
  - else add/update `requirements.txt`
  - do not add both

## file-level scope (hard constraint)

allowed to create/modify only:

- `niels_gpt/tokenizer.py`
- `niels_gpt/chat_template.py`
- `scripts/train_tokenizer.py`
- `tests/test_tokenizer.py`
- `tests/test_chat_template.py`
- dependency file (`pyproject.toml` OR `requirements.txt`)

do not touch anything else.

## artifacts written to disk

tokenizer outputs live under:

- `artifacts/tokenizer/spm.model`
- `artifacts/tokenizer/tokenizer_meta.json`

`tokenizer_meta.json` must include at least:
- `vocab_size` (int)
- `model_type` (string: "unigram")
- `special_tokens` mapping token string -> id (sys/usr/asst/eot)
- `seed` (int)
- `sentencepiece_version` (string)
- `input_globs` (list[str])
- `input_files` (list[str], sorted, repo-relative or absolute but consistent)
- `inputs_sha256` (sha256 of concatenated file bytes in deterministic order)

note: determinism is best-effort; meta is for provenance + debugging.

## public python interfaces

### tokenizer

`niels_gpt/tokenizer.py`

```python
from __future__ import annotations
from dataclasses import dataclass
from typing import Iterable, Sequence

import torch

SPECIAL_TOKENS = ("<|sys|>", "<|usr|>", "<|asst|>", "<|eot|>")

@dataclass(frozen=True)
class TokenizerPaths:
    model_path: str  # .../spm.model
    meta_path: str   # .../tokenizer_meta.json

class SentencePieceTokenizer:
    def __init__(self, model_path: str):
        ...

    @property
    def vocab_size(self) -> int:
        ...

    def encode(self, text: str) -> list[int]:
        """returns token ids (python ints)."""

    def encode_torch(self, text: str, *, device: str | None = None) -> torch.LongTensor:
        """returns shape (n,), dtype int64."""

    def decode(self, ids: Sequence[int] | torch.Tensor) -> str:
        """decodes ids into text."""

    def special_token_ids(self) -> dict[str, int]:
        """returns ids for SPECIAL_TOKENS by string key."""

def load_tokenizer(model_path: str) -> SentencePieceTokenizer:
    ...

invariants:
	•	encoding each special token string returns exactly 1 id
	•	special_token_ids() contains keys for all 4 special tokens

chat template

niels_gpt/chat_template.py

message schema:
	•	each message is {"role": "system"|"user"|"assistant", "content": str}

template (exact structure):
	•	each message becomes:
	•	<|sys|> {content} <|eot|> for system
	•	<|usr|> {content} <|eot|> for user
	•	<|asst|> {content} <|eot|> for assistant

interfaces:

from __future__ import annotations
from typing import Literal, TypedDict

from niels_gpt.tokenizer import SentencePieceTokenizer

class Message(TypedDict):
    role: Literal["system", "user", "assistant"]
    content: str

def format_chat(
    tok: SentencePieceTokenizer,
    messages: list[Message],
) -> list[int]:
    """returns token ids for a full conversation with <|eot|> after each message."""

def format_prompt(
    tok: SentencePieceTokenizer,
    messages: list[Message],
) -> list[int]:
    """
    like format_chat, but for generation:
    - includes all prior messages as completed turns (each ends with <|eot|>)
    - appends '<|asst|>' only at the end (no <|eot|>) so the model continues as assistant
    """

def extract_assistant_reply(
    tok: SentencePieceTokenizer,
    generated_ids: list[int],
) -> list[int]:
    """
    given full generated token ids (including prompt),
    return the assistant reply tokens:
    - tokens after the final <|asst|> token
    - up to (but not including) the first <|eot|> after that
    - if no <|eot|> exists, return tokens to end
    """

hard invariants:
	•	format_prompt(...) ends with <|asst|> id
	•	extract_assistant_reply never returns tokens before the last <|asst|>

cli script

scripts/train_tokenizer.py

requirements:
	•	supports multiple --input_glob args
	•	deterministic ordering: expand globs, sort paths
	•	reads each file as utf-8 with errors="replace", normalizes \r\n -> \n
	•	trains sentencepiece unigram with:
	•	vocab size = 16000
	•	byte fallback on
	•	reserve the 4 special tokens so each is a single id
	•	writes spm.model + tokenizer_meta.json into --out_dir

flags (exact):
	•	--out_dir (default artifacts/tokenizer)
	•	--vocab_size (default 16000)
	•	--seed (default 42)
	•	--model_type (default “unigram”, only allowed value in v0)
	•	--input_glob (repeatable; required at least once)

exit with nonzero + clear message if:
	•	no files matched
	•	any special token is not single-id after training

tests

tests/test_tokenizer.py
	•	trains a tiny tokenizer into a temp dir from 2–3 tiny input files
	•	asserts:
	•	each special token encodes to length 1
	•	special_token_ids() returns distinct ints for the 4 tokens
	•	encode/decode roundtrip works on simple english

tests/test_chat_template.py
	•	trains tiny tokenizer in temp dir (or reuses fixture)
	•	builds a message list with system/user/assistant
	•	asserts:
	•	format_chat contains <|eot|> after every message
	•	format_prompt ends with <|asst|> and no trailing <|eot|>
	•	extract_assistant_reply slices correctly given a synthetic generated sequence:
prompt + <|asst|> + “hello” + <|eot|> + junk

acceptance commands

from repo root:
	1.	train tokenizer from your real data:

python scripts/train_tokenizer.py \
  --out_dir artifacts/tokenizer \
  --input_glob ".roam-data/**/*.md" \
  --input_glob "data/primer.txt"

	2.	run tests:

pytest -q

done when both commands succeed.
