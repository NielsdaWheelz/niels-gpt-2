# pr-07: mixed precision on mps + activation checkpointing (no behavior regressions)

goal
- add optional mixed precision training on apple mps via autocast (fp16 default).
- add optional activation checkpointing at the transformer block level to reduce memory.
- preserve existing semantics: `step` == optimizer update, eval/checkpoint cadence keyed on that step.

non-goals
- no model architecture changes (other than wrapping blocks for checkpointing when enabled).
- no dataset/cache changes.
- no inference / kv-cache / server changes.
- no cuda support, no gradscaler (repo supports {cpu,mps} only).

## decisions locked

amp
- enable autocast only when:
  - train_cfg.amp == true AND device == "mps"
- gradscaler:
  - never use torch.cuda.amp.GradScaler (no cuda in repo)
- dtype:
  - train_cfg.amp_dtype in {"fp16","bf16"}
  - default "fp16"
  - if requested dtype not supported on mps for the ops used, fail fast with a clear error recommending amp=false or amp_dtype="fp16"

activation checkpointing
- enable only when:
  - train_cfg.activation_checkpointing == true AND model.training == true
- granularity:
  - checkpoint each Block in `self.blocks` (nn.ModuleList)
- implementation:
  - `torch.utils.checkpoint.checkpoint(block, x, use_reentrant=False)`
- tracing compatibility:
  - if the model forward is invoked in a mode that requests traces/attn/topk/etc (anything that changes return type or requires side-channel tensors), checkpointing must be disabled for that forward call (fallback to normal block calls). training code should not request traces; this is defensive.

grad accumulation (already exists)
- keep existing accumulation structure.
- invariants to enforce (if not already true):
  - loss is divided by accum_steps before backward
  - gradients are clipped once per optimizer step (not per microbatch)
  - optimizer.step happens once per outer `step`
  - lr schedule advances once per optimizer step (not per microbatch)

## config surface area

edit: train/config.py
- TrainCfgSchema: add fields (with defaults)
  - amp: bool = True
  - amp_dtype: Literal["fp16","bf16"] = "fp16"
  - activation_checkpointing: bool = False

notes
- keep existing fields and semantics:
  - B is microbatch size
  - accum_steps exists already
- pydantic defaults must allow older configs to load without specifying new keys.

edit: configs/*.json
- update all shipped training configs (smoke + full, pretrain + sft + pipeline) to explicitly include:
  - "amp": true
  - "amp_dtype": "fp16"
  - "activation_checkpointing": false
- do not change any other values in those configs.

## code changes (expected files)

allowed files to modify
- train/config.py
- train/run.py (only to plumb config + log flags; do not redesign cli)
- train/pretrain.py (or wherever the phase loop lives)
- train/sft.py (same)
- train/train_loop.py (if you have a shared loop module; otherwise the phase files)
- niels_gpt/model/gpt.py (only to add checkpointing wrapper in forward)
- niels_gpt/model/blocks.py (only if required by signature constraints; prefer not)
- train/checkpointing.py (only if needed to include new config keys; ideally no change)
- configs/*.json
- tests/* (new tests)

forbidden
- no changes to web server / ui / inspect endpoints
- no changes to dataset definitions, token cache formats, or streaming logic
- no changes to checkpoint key schema unless strictly required (should not be)

## implementation requirements

### amp wrapper
- create a small helper in train (or reuse existing utilities) that returns a context manager:

  - `amp_ctx = torch.autocast(device_type="mps", dtype=desired_dtype)` if enabled
  - otherwise `contextlib.nullcontext()`

- wrap forward+loss computation in that context:
  - forward pass
  - loss computation
  - (optionally) logit softmax not needed; keep as-is

- backprop:
  - call `loss.backward()` as usual
  - no scaler

### activation checkpointing in gpt forward
- in GPT.forward:
  - if checkpointing is enabled AND self.training AND traces not requested:
    - iterate blocks with checkpoint(block, x, use_reentrant=False)
  - else:
    - normal `x = block(x, ...)`

### logging
during training startup, print one line that includes:
- device
- amp enabled? dtype
- activation_checkpointing enabled?
- micro_B (B), accum_steps, effective batch = B * accum_steps

## acceptance tests

add tests that run fast on cpu; mps tests should be skipped if unavailable.

1) config defaults
- loading an old json that lacks the new keys still succeeds
- TrainCfgSchema has correct defaults: amp=True, amp_dtype="fp16", activation_checkpointing=False

2) amp context selection
- with device="cpu": autocast is NOT used even if amp=true
- with device="mps" (if available): autocast is used when amp=true

3) training smoke (cpu)
- run a tiny training loop for 2 optimizer steps (with accum_steps >= 2) on cpu:
  - assert loss is finite
  - assert step counter increments by 1 per optimizer update
  - assert optimizer.step called exactly `total_steps` times (can be inferred from saved step)

4) activation checkpointing smoke (cpu)
- same as (3) but with activation_checkpointing=true
- assert loss finite and no exception

5) mps amp smoke (optional)
- if mps available:
  - run 1 optimizer step with amp=true, amp_dtype="fp16"
  - assert loss finite and no exception

6) checkpoint/resume compatibility
- run 1 step, save checkpoint
- resume from it and run 1 more step
- assert resumed step starts at previous_step+1 and training continues
- ensure new config fields are preserved in saved train_cfg (or can be reloaded without mismatch)

## done definition

pr is done when:
- all tests pass on cpu.
- on a mac with mps available, a 50-step smoke run completes with amp=true (fp16) and produces checkpoints.
- disabling amp yields identical behavior to before pr-7 (no crashes; loss finite; cadence unchanged).
