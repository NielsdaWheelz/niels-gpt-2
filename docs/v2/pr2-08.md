pr-08 spec — kv-cache inference (prefill + decode) with attn row tracing

**STATUS: IMPLEMENTED ✓**

goal

implement kv-cache decoding so generation no longer recomputes the full forward pass every token. keep the existing “manual attention” math, keep RoPE, and preserve attention tracing (attn row for last token at a selected layer).

hard constraints
	•	no training changes (no changes to loss, optimizer, datasets, batching, etc.)
	•	no architecture changes (no gqa/mqa/mla, no new norms/activations)
	•	no tokenizer changes
	•	no substring stop hacks: stopping is by <|eot|> token id only
	•	model must remain usable via existing GPT.forward(x) and GPT.forward_with_attn_trace(...)
	•	kv-cache must be managed externally (no model-global mutable cache)

non-goals
	•	sliding window / cache eviction
	•	serving/web/sse formats (not present; do not add)
	•	performance micro-optimizations beyond kv-cache correctness

⸻

definitions / invariants

shapes
	•	B: batch size (generation uses B=1 but implementation must support B>=1)
	•	t: current sequence length (tokens processed so far)
	•	T_max: model context length (cfg.T, v1 expects 1024)
	•	H: num heads
	•	D: head dim (C/H)
	•	k/v cache stored per layer in head space.

kv-cache representation (locked)

use preallocated fixed buffers:

from dataclasses import dataclass
import torch

@dataclass
class KVCache:
    k: torch.Tensor  # (L, B, H, T_max, D)
    v: torch.Tensor  # (L, B, H, T_max, D)
    t: int           # number of cached timesteps already filled (0..T_max)

    @property
    def L(self) -> int: ...
    @property
    def T_max(self) -> int: ...

cache dimension checks (required)

implement strict shape and bounds validation to prevent silent bugs:

in prefill:
	•	assert prompt_ids.shape[0] == cache.k.shape[1], "batch size mismatch: prompt B must equal cache B dimension"
	•	assert cache.t == 0, "cache must be empty (t=0) before prefill"
	•	t0 = prompt_ids.shape[1]
	•	assert t0 <= cache.T_max, f"prompt length {t0} exceeds T_max {cache.T_max}"
	•	after filling cache: assert cache.t == t0

in decode_step:
	•	assert last_token_ids.shape == (B, 1), "decode_step requires exactly (B, 1) input"
	•	assert last_token_ids.shape[0] == cache.k.shape[1], "batch size mismatch"
	•	assert cache.t < cache.T_max, f"cache overflow: t={cache.t} must be < T_max={cache.T_max}"
	•	after appending: assert cache.t <= cache.T_max

in generate_ids_cached (or helpers):
	•	assert len(prompt_ids) + max_new_tokens <= T_max, raise ValueError with clear message
	•	during decode loop: if cache.t >= T_max before decode_step, raise ValueError("exceeded T_max during generation")

dtype/device rules (locked)

cache dtype must exactly match the K/V tensors produced during inference to prevent silent casts or slowdowns (especially on mps).

dtype selection (pick one approach and lock it):
	•	OPTION A (simple, recommended for v1): inference always runs in fp32, allocate cache with dtype=torch.float32
	•	OPTION B (if you want fp16 now): generate_ids_cached must determine dtype explicitly:
	•	add infer_dtype: torch.dtype | None = None parameter (default None = fp32)
	•	if infer_dtype is None: use torch.float32
	•	if infer_dtype is provided: use that dtype and wrap inference in torch.autocast(device_type=..., dtype=infer_dtype) if needed
	•	allocate_kv_cache(..., dtype=determined_dtype)

allocation and assignment rules:
	•	allocate_kv_cache must use the exact dtype passed in
	•	all K/V writes into cache must match cache.k.dtype and cache.v.dtype exactly
	•	assert cache.k.dtype == k_proj.dtype before assignment (optional but recommended for debugging)
	•	device must match model device

logits/sampling:
	•	logits/top-k computations may cast to fp32 for numerical stability (allowed and recommended)

layer indexing (locked)

layer_idx source of truth:
	•	model.blocks is an nn.ModuleList of length L
	•	layer_idx is the index into model.blocks (range 0..L-1)
	•	do NOT infer layer count from config alone - use len(model.blocks)
	•	prefill and decode_step must iterate layers in order: for layer_idx, block in enumerate(model.blocks)
	•	cache dimension 0 corresponds to layer_idx

rope position semantics (locked)
	•	RoPE applied to q and k only
	•	store post-rope keys in cache
	•	positions are absolute within the context window:
	•	prefill uses positions 0..t0-1
	•	decode step uses position pos = cache.t for the new token

max sequence length policy (locked)

hard cap. do not implement sliding window.

overflow behavior (LOCKED - no choice):
	•	if len(prompt_ids) > T_max: raise ValueError("prompt length exceeds T_max")
	•	if len(prompt_ids) + max_new_tokens > T_max: raise ValueError at function entry (do NOT partially generate then error)
	•	during decode loop: if cache.t >= T_max before calling decode_step, raise ValueError("exceeded T_max during generation")
	•	NEVER stop early silently - always raise ValueError on overflow

⸻

required behavior

1) prefill + decode API (new inference functions)

create niels_gpt/infer/kv_cache.py with these functions:

def allocate_kv_cache(
    *,
    L: int,
    B: int,
    H: int,
    T_max: int,
    D: int,
    device: str,
    dtype: torch.dtype,
) -> KVCache:
    """returns zero-initialized (or uninitialized + t=0) KVCache on device/dtype."""

@torch.no_grad()
def prefill(
    model: "GPT",
    prompt_ids: torch.LongTensor,  # (B, t0)
    cache: KVCache,
    *,
    trace_layer: int | None = None,
    return_attn_row: bool = False,
) -> tuple[torch.Tensor, KVCache, dict | None]:
    """
    runs the model over the full prompt, fills cache up to t0.

    INVARIANT: model must be in eval mode (model.eval()) before calling.
    This function does NOT call model.eval() - caller is responsible.

    returns:
      logits: (B, t0, V)
      cache: same object with cache.t == t0 and k/v filled for all layers
      trace: optional dict with:
        - "layer": int
        - "attn_row": (B, H, t0) attention probs for the LAST prompt token
    """

and:

@torch.no_grad()
def decode_step(
    model: "GPT",
    last_token_ids: torch.LongTensor,  # (B, 1)
    cache: KVCache,
    *,
    trace_layer: int | None = None,
    return_attn_row: bool = False,
) -> tuple[torch.Tensor, KVCache, dict | None]:
    """
    consumes exactly one token, appends its K/V into cache at position cache.t,
    and returns logits for this token position.

    INVARIANT: model must be in eval mode (model.eval()) before calling.
    This function does NOT call model.eval() - caller is responsible.

    returns:
      logits: (B, 1, V)
      cache: cache.t increased by 1
      trace: optional dict with:
        - "layer": int
        - "attn_row": (B, H, cache.t) attention probs for THIS new token attending to all cached keys
    """

masking behavior (locked)
	•	prefill uses causal masking exactly like today (triangular)
	•	decode_step uses query_len=1; no triangular mask required. the new token can attend to all cached keys (including itself).

⸻

2) minimal modifications to attention/block code

you currently have manual attention in niels_gpt/model/blocks.py and Block.forward(x, return_attn=False).

you must add support for THREE execution modes:

mode a: normal (existing)
	•	input x: (B, t, C)
	•	compute q/k/v for full t
	•	apply RoPE over positions 0..t-1
	•	compute scores (B,H,t,t) + causal mask
	•	return output (B, t, C) and (optional) full attn_probs or last row according to existing tracing

mode b: cached prefill (new)
	•	input x: (B, t0, C) (full prompt)
	•	compute q/k/v for all t0 tokens
	•	apply RoPE over positions 0..t0-1
	•	compute scores (B,H,t0,t0) + causal mask (same as mode a)
	•	write ALL k/v into cache at [layer_idx, :, :, 0:t0, :] in ONE ASSIGNMENT (not a python loop over tokens)
	•	softmax → attn_probs (B,H,t0,t0) (pre-dropout)
	•	output (B,t0,C)
	•	if tracing this layer: produce attn_row = attn_probs[:, :, -1, :] shape (B,H,t0)
	•	attn_row is for the LAST prompt token (index -1) attending over all t0 keys (positions 0..t0-1)
	•	attn_row must come from the same computation used for output (not recomputed)
	•	attn_row is PRE-DROPOUT (use probs before applying dropout, even though dropout should be off in eval)

mode c: cached decode (new)
	•	input x: (B, 1, C) (single token)
	•	compute q/k/v for that single token (head space)
	•	apply RoPE at position pos = cache.t (single pos)
	•	append k/v into cache at [layer_index, :, :, pos, :] (cache.t is incremented AFTER this)
	•	build k_all = cache.k[layer, :, :, :pos+1, :] and same for v (includes newly added key at pos)
	•	compute scores (B,H,1,pos+1) with no causal mask
	•	softmax → attn_probs (B,H,1,pos+1) (pre-dropout)
	•	output (B,1,C)
	•	if tracing this layer: produce attn_row = attn_probs[:, :, 0, :] shape (B,H,pos+1)
	•	attn_row is for the NEW token attending over cache.t keys AFTER the append (positions 0..pos inclusive, total pos+1 keys)
	•	cache.t is incremented to pos+1 after the append
	•	attn_row must come from the same computation used for output (not recomputed)
	•	attn_row is PRE-DROPOUT

important: tracing uses pre-dropout probabilities (match your current behavior). dropout should be disabled in eval anyway, but return the pre-dropout values.

implementation requirement

do not contort your existing Block.forward signature into a mess.

add TWO separate entry points to minimize blast radius and prevent math duplication:
	•	keep Block.forward(x, return_attn=False) unchanged for training/inference full forward
	•	add Block.prefill(x, cache: KVCache, layer_idx: int, return_attn_row: bool=False) -> (out, attn_row | None)
	•	add Block.decode_step(x_one, cache: KVCache, layer_idx: int, pos: int, return_attn_row: bool=False) -> (out, attn_row | None)

alternatively, add these methods to SelfAttention instead of Block (either is acceptable, but both must exist).

critical: prefill and decode_step must share the same underlying attention computation to prevent divergence. do not duplicate the q/k/v projection or attention score math

⸻

3) generation API changes

you currently have generation that full-recomputes per token in niels_gpt/generate.py.

add a new function:

@torch.no_grad()
def generate_ids_cached(
    model: "GPT",
    prompt_ids: list[int],
    *,
    max_new_tokens: int,
    eot_token_id: int,
    temperature: float = 0.9,
    top_k: int | None = 50,
    trace_layer: int | None = None,
) -> dict:
    """
    returns a dict with:
      - "ids": list[int] full sequence (prompt + generated, including <|eot|> if generated)
      - "steps": list[dict] each step optionally contains:
          - "token_id": int
          - "top_k": list[tuple[int, float]] or richer structure
          - "entropy": float
          - "attn_row": list[list[float]]  # per head, only if trace_layer is set

    INVARIANT: calls model.eval() at entry to ensure deterministic inference (no dropout).
    """

requirements:
	•	call model.eval() at function entry
	•	use prefill once on the prompt to fill cache
	•	then loop max_new_tokens decode steps
	•	stop when generated token == eot_token_id
	•	enforce hard cap len(prompt)+max_new_tokens <= T_max (raise ValueError)

decoding behavior (locked for tests)

implement greedy decoding path for exact equivalence tests:

def generate_ids_greedy_cached(...) -> list[int]
def generate_ids_greedy_full(...) -> list[int]  # baseline using existing full forward loop

greedy = argmax(logits_last).

CRITICAL: baseline greedy_full semantics must match cached path for equivalence tests:
	•	greedy_full must NOT crop to last T_max tokens
	•	greedy_full must enforce the same hard cap: if len(prompt) + max_new_tokens > T_max, raise ValueError
	•	greedy_full recomputes forward pass on the full prefix each step, but ONLY while total_len <= T_max
	•	both paths must stop at the same conditions: eot_token_id or max_new_tokens or T_max overflow

sampling path (temperature/top-k) is allowed but is not used for equivalence tests.

⸻

4) inspection full attention (optional but clean)

since you already have forward_with_attn_trace(attn_full=...), add a helper:

@torch.no_grad()
def full_attention_matrix(
    model: "GPT",
    ids: list[int],
    *,
    trace_layer: int,
    device: str,
) -> torch.Tensor:
    """
    runs forward_with_attn_trace on the full sequence once and returns
    attn_full for that layer: (1, H, t, t)
    """

this is not streamed; it’s on-demand debugging.

⸻

acceptance tests (must exist)

create tests/test_kv_cache.py with:
	1.	greedy equivalence (cpu)

	•	instantiate a tiny model config (can be smaller than your real one for speed, but must exercise rope + masking)
	•	call model.eval() before running any inference
	•	seed everything
	•	random prompt ids length t0 (>=2), random max_new_tokens (e.g. 16)
	•	compare outputs:
	•	greedy_full(prompt) == greedy_cached(prompt) exact list equality

	2.	attn row shape + normalization

	•	run cached decode with trace_layer=0 on a small prompt and 1 decode step
	•	assert attn_row shape == (B, H, cache.t) where cache.t is the number of keys after decode
	•	for each batch and head: assert abs(sum(attn_row[b, h, :]) - 1.0) < 1e-4, "attention row must sum to 1"
	•	assert all entries are finite: torch.isfinite(attn_row).all()
	•	assert all entries in valid range: (attn_row >= -1e-6).all() and (attn_row <= 1.0 + 1e-6).all()

	3.	stop-on-eot (deterministic test)

	•	do NOT rely on stochastic model output
	•	create a DummyModel (or mock) that returns logits with argmax = eot_token_id on the first decode step
	•	call generate_ids_greedy_cached with this model
	•	assert: output ids end with eot_token_id
	•	assert: generation stopped after emitting eot (length == len(prompt) + 1)
	•	this isolates the control flow logic from model stochasticity

	4.	hard cap enforcement

	•	if len(prompt)+max_new_tokens > T_max, generate_ids_cached raises ValueError.

optional:
	•	test_mps_equivalence_if_available guarded by torch.backends.mps.is_available().

⸻

done means
	•	greedy cached generation matches greedy full generation exactly (cpu; mps if available)
	•	generation is noticeably faster for non-trivial prompts (not measured by tests, but should be obvious)
	•	no changes to training behavior or model forward interfaces used by training
	•	tracing still works: attn_row for last token at selected layer is correct and normalized
