pr-03: token-id cache + mmap datasets (pretrain + sft)

goal

build a reproducible on-disk token cache (uint16 token ids) and dataset loaders that read those caches efficiently (mmap) for training.
	•	pretrain cache: random-window sampling over large token streams
	•	sft cache: example-aware sequences + assistant-only loss masking

fineweb-edu must be supported via huggingface datasets streaming (no full download), then cached locally as token ids.

non-goals
	•	no model changes
	•	no trainer changes
	•	no changes to tokenizer training (pr-01) or dataset parsing (pr-02), except calling those loaders
	•	no “clever” data cleaning beyond light normalization already agreed
	•	no moe/mla/gqa/etc

fixed defaults (must be constants with CLI overrides)

tokenizer:
	•	sentencepiece tokenizer already exists (from pr-01)
	•	vocab size ~16k + 4 special tokens
	•	special tokens: <|ngpt_sys_84a5023f67d74cf29cc4001becde983c|> <|ngpt_usr_84a5023f67d74cf29cc4001becde983c|> <|ngpt_asst_84a5023f67d74cf29cc4001becde983c|> <|ngpt_eot_84a5023f67d74cf29cc4001becde983c|>

fineweb-edu:
	•	dataset id: HuggingFaceFW/fineweb-edu
	•	config: CC-MAIN-2024-10
	•	streaming: streaming=True
	•	shuffle: dataset.shuffle(seed=seed, buffer_size=10_000)
	•	token budgets:
	•	train: 200_000_000 tokens
	•	val: 5_000_000 tokens

wikitext:
	•	cache all (train/val), used for smoke/eval (not required to be in phase-a mix)

roam:
	•	include all roam docs
	•	split by file list (seeded) as implemented in pr-02

sft:
	•	datasets: dolly-15k + oasst1
	•	val split: val_frac=0.10 (by example, seeded)

cache format:
	•	token dtype: little-endian uint16
	•	shard size (pretrain): 128 MB per shard target
	•	sft: single *_tokens.bin + *_idx.npy offsets array

seed:
	•	uses repo-wide seed, but cache builder also accepts explicit seed (default 42)

repo layout (must match exactly)

add:

niels_gpt/cache/
  __init__.py
  build_cache.py
  formats.py
  meta.py
  mmap.py
  pretrain_dataset.py
  sft_dataset.py
  cli.py

cache/                         # generated artifacts (gitignored)
  tokenizer/
    spm.model                  # copy/symlink from pr-01 output
    meta.json
  pretrain/
    fineweb_edu/
      train/ shard_00000.bin ...
      val/   shard_00000.bin ...
      meta.json
    roam/
      train/...
      val/...
      meta.json
    wikitext/
      train/...
      val/...
      meta.json
    gutenberg/
      train/...
      val/...
      meta.json
  sft/
    dolly15k/
      train_tokens.bin
      train_idx.npy
      val_tokens.bin
      val_idx.npy
      meta.json
    oasst1/
      ...

add tests:

tests/test_cache_determinism.py
tests/test_pretrain_window_sampling.py
tests/test_sft_masking.py
tests/test_sft_cache_roundtrip.py

public interfaces (must implement exactly)

cache builders

# niels_gpt/cache/build_cache.py

from collections.abc import Iterable, Iterator
from typing import TypedDict, Literal

class Message(TypedDict):
    role: Literal["system", "user", "assistant"]
    content: str

def build_pretrain_cache(
    texts: Iterable[str],
    out_dir: str,
    *,
    tokenizer,
    max_train_tokens: int,
    max_val_tokens: int,
    shard_bytes: int,
    seed: int,
    shuffle_buffer: int | None,
) -> None:
    """
    writes:
      out_dir/train/shard_*.bin
      out_dir/val/shard_*.bin
      out_dir/meta.json
    token dtype: uint16 little-endian
    deterministic given seed + identical input stream order.
    """

def build_sft_cache(
    examples: Iterable[list[Message]],
    out_dir: str,
    *,
    tokenizer,
    val_frac: float,
    seed: int,
) -> None:
    """
    writes:
      out_dir/train_tokens.bin, out_dir/train_idx.npy
      out_dir/val_tokens.bin,   out_dir/val_idx.npy
      out_dir/meta.json
    where idx is offsets into tokens.bin (int64), offsets-only (length inferred).
    deterministic given seed + identical input stream order.
    """

datasets (training-time sampling)

# niels_gpt/cache/pretrain_dataset.py

class PretrainTokenStreamDataset:
    def __init__(self, shards_dir: str, *, T: int, device: str):
        """
        shards_dir contains shard_*.bin (uint16).
        must memory-map shards; do not load all tokens into RAM.
        """

    def get_batch(self, *, B: int, generator: "torch.Generator") -> tuple["torch.LongTensor","torch.LongTensor"]:
        """
        returns x,y of shape (B,T) on device, dtype int64
        invariant: y[:, :-1] == x[:, 1:]
        samples random windows within a randomly chosen shard.
        """

# niels_gpt/cache/sft_dataset.py

class SFTExampleDataset:
    def __init__(self, tokens_path: str, idx_path: str, *, T: int, device: str, eot_id: int):
        """
        tokens_path: uint16 tokens concatenated
        idx_path: int64 offsets array
        """

    def get_batch(self, *, B: int, generator: "torch.Generator") -> tuple["torch.LongTensor","torch.LongTensor","torch.LongTensor"]:
        """
        returns (x, y, y_masked) where:
          x: (B,T) int64
          y: (B,T) int64 (next-token targets)
          y_masked: (B,T) int64 where non-assistant targets are -100 (ignore_index)

        packing/truncation:
          - take one full example sequence of token ids
          - if longer than T+1: truncate to first T+1 tokens
          - if shorter than T+1: pad by appending eot_id until length T+1
          - then x=seq[:-1], y=seq[1:]

        masking rule (SFT):
          - compute loss only on tokens that are inside assistant spans:
            tokens after the assistant sentinel up to and including the next EOT
          - all other y positions in y_masked must be -100.
        """

meta contract

# niels_gpt/cache/meta.py

def write_meta(path: str, meta: dict) -> None: ...
def read_meta(path: str) -> dict: ...
def sha256_file(path: str) -> str: ...

each meta.json must include at least:
	•	dataset_name, dataset_config (or null), split_rule
	•	seed
	•	token_dtype (“uint16-le”)
	•	tokenizer_sha256, vocab_size
	•	special_token_ids: sys/usr/asst/eot
	•	totals: train_tokens, val_tokens for pretrain; train_examples, val_examples for sft
	•	shard_bytes for pretrain

cli

a single entrypoint:

python -m niels_gpt.cache.cli build-all --cache-dir cache --seed 42

it must build caches for:
	•	pretrain: fineweb-edu (streaming), roam, wikitext
	•	sft: dolly15k, oasst1
(gutenberg may be a stub: create directories + meta with “not built” flag, but don’t fail)

determinism requirements
	•	splitting and shuffling must be deterministic given seed
	•	randomness must use torch RNG only (no random / numpy RNG)
	•	for streaming fineweb-edu, determinism is “best effort” given upstream stream stability:
	•	use shuffle(seed, buffer_size) and then consume in order
	•	record in meta that source is streaming

performance requirements
	•	never materialize fineweb-edu into memory
	•	token cache writers must stream-write
	•	dataset readers must memory-map (mmap), not load entire .bin into RAM

acceptance tests

must pass:
	1.	sft masking correctness
hand-construct a tiny token sequence with <|ngpt_sys_84a5023f67d74cf29cc4001becde983c|>…<|ngpt_eot_84a5023f67d74cf29cc4001becde983c|><|ngpt_usr_84a5023f67d74cf29cc4001becde983c|>…<|ngpt_eot_84a5023f67d74cf29cc4001becde983c|><|ngpt_asst_84a5023f67d74cf29cc4001becde983c|>A B <|ngpt_eot_84a5023f67d74cf29cc4001becde983c|> and verify:

	•	only targets for A, B, and EOT are not -100
	•	everything else is -100

	2.	pretrain window sampling invariant
y[:, :-1] == x[:, 1:] always, and ids are within vocab size.
	3.	cache roundtrip
write a small cache from a fixed list of texts/examples and verify:

	•	tokens file parses as uint16
	•	idx offsets are valid
	•	reconstructed sequences match original tokenization

	4.	determinism (small synthetic input)
given the same small in-memory iterable + same seed, cache files are byte-identical (compare sha256 of outputs).

constraints
	•	do not change PR-01 tokenizer API or PR-02 loader APIs; adapt via wrappers inside this PR if needed.
	•	no training loop code in this PR.
	•	keep all constants overridable via CLI flags.
