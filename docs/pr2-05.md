pr-05 spec: backbone v1 (rmsnorm + swiglu) + require V

goal

upgrade the transformer backbone to v1 by:
	•	replacing LayerNorm → RMSNorm (eps=1e-5, no bias)
	•	replacing GELU MLP → SwiGLU MLP (hidden width d_ff, default 1536)
	•	making ModelConfig.V required (no default), and fixing any instantiations missing V
	•	preserving all existing public interfaces and attention-trace behavior

non-goals (hard exclusions)

do not implement or modify:
	•	training loop logic, loss, optimizer, schedule, batching, caching
	•	rope implementation / cache shape / registration behavior
	•	kv-cache (later PR)
	•	attention variant changes (no gqa/mqa), no flash attention, no fused kernels
	•	output formatting, stop sequence hacks (already replaced by the EOT sentinel in earlier PRs)
	•	changes to GPT.forward signature or return type

allowed files

may modify
	•	niels_gpt/config.py
	•	niels_gpt/model/blocks.py
	•	niels_gpt/model/gpt.py (wiring only, no signature changes)
	•	any repo files that instantiate ModelConfig()/ModelConfig( without passing V (scripts/tests)

must not modify
	•	niels_gpt/model/rope.py
	•	training scripts except for config instantiation fixes
	•	web server/UI code

repository facts (must preserve)

model entrypoint and signatures
	•	niels_gpt/model/gpt.py defines class GPT
	•	constructor: GPT(cfg: ModelConfig)
	•	GPT.forward(x) -> logits where:
	•	x: (B,t) int64 token ids
	•	logits: (B,t,V) float32

attention tracing contract (must remain exact)

GPT.forward_with_attn_trace(x, trace_layer: int, return_full_attn: bool = False)
returns (logits, AttnTrace) where:
	•	AttnTrace["layer"]: int
	•	AttnTrace["attn_row"]: (B,H,t) float32, post-softmax probs pre-dropout, same device as model
	•	AttnTrace["attn_full"]: (B,H,t,t) float32 or None (same meaning)

mechanism (must remain compatible):
	•	selected block(h, return_attn=True) returns (h, attn_probs)
	•	attention returns (out, attn_probs) where attn_probs is (B,H,t,t) softmax output before attention dropout

RoPE (must remain unchanged)
	•	niels_gpt/model/rope.py
	•	rope_cache(...) -> (sin,cos) shapes (1,1,T,D//2)
	•	apply_rope(q,k,sin,cos) expects q,k: (B,H,t,D)
	•	cache is registered as a non-persistent buffer in CausalSelfAttention.__init__

dropout placement (keep exactly current semantics)

keep “residual-branch dropout”:

x = x + dropout(attn_out)   # dropout BEFORE add
x = x + dropout(mlp_out)    # dropout BEFORE add

also keep:
	•	embedding dropout on token embeddings
	•	attention dropout applied to probs after softmax for the actual attention output path
	•	returned attn_probs for tracing is pre-dropout softmax

init (keep exactly current)
	•	Linear/Embedding weights: Normal(0, 0.02)
	•	norm weights: ones, norm bias (if any) zeros

weight tying must remain:
	•	lm_head has no bias
	•	lm_head.weight is tied to token embedding weight

⸻

decisions locked for this PR
	•	hard assert stays: input length t <= cfg.T inside forward paths
	•	RMSNorm: eps 1e-5, no bias
	•	SwiGLU hidden width: cfg.d_ff, default 1536
	•	attention outputs and tracing semantics unchanged
	•	ModelConfig.V becomes required (no default)

⸻

required config changes

edit niels_gpt/config.py:
	1.	make V required

	•	remove default value for V
	•	constructor must fail if V is not provided

	2.	update v1 defaults for other model dims (unless you already set them elsewhere)

	•	T=1024
	•	C=512
	•	L=8
	•	H=8
	•	d_ff=1536
	•	dropout=0.1
	•	rope_theta=10000.0

	3.	if there are places relying on ModelConfig() without args, fix them in this PR (tests, scripts, etc.)

⸻

implementation requirements

RMSNorm

add RMSNorm implementation either in blocks.py or a new helper module (prefer new helper if blocks.py is getting fat).

must implement:
	•	RMSNorm(dim: int, eps: float = 1e-5)
	•	parameters:
	•	weight: nn.Parameter shape (dim,)
	•	no bias parameter
	•	forward for x shape (..., dim):
	•	rms = sqrt(mean(x^2, dim=-1, keepdim=True) + eps)
	•	y = x / rms * weight

SwiGLU MLP

replace GELU MLP with SwiGLU:

for input x: (B,t,C):
	•	a = W_a(x) + b_a to (B,t,d_ff)
	•	g = W_g(x) + b_g to (B,t,d_ff)
	•	h = silu(g) * a
	•	out = W_o(h) + b_o to (B,t,C)

notes:
	•	d_ff is taken from cfg.d_ff (default 1536)
	•	use torch.nn.functional.silu

transformer block

in niels_gpt/model/blocks.py, update the block to:
	•	pre-norm remains
	•	both norm layers become RMSNorm
	•	mlp becomes SwiGLU

preserve:
	•	attention module interface and trace path (return_attn=True still returns attn_probs)
	•	dropout placement

⸻

required test coverage (add if missing)

use pytest. add/update tests under existing tests folder (whatever it is in your repo; do not invent a new framework).

tests to include
	1.	forward shape & dtype

	•	instantiate a tiny cfg for test (small dims are fine) but include required V
	•	run GPT(cfg)(x) and assert logits shape (B,t,V) and dtype float32

	2.	trace contract

	•	call forward_with_attn_trace(..., return_full_attn=False) and assert:
	•	attn_row.shape == (B,H,t)
	•	attn_row.dtype == torch.float32
	•	call with return_full_attn=True and assert:
	•	attn_full.shape == (B,H,t,t)
	•	attn_full.sum(dim=-1) is approximately 1.0 (tolerance 1e-4 to 1e-3)

	3.	rmsnorm has no bias

	•	assert the RMSNorm module does not define a .bias parameter

	4.	swiglu projection shapes

	•	for cfg C=512, d_ff=1536, ensure mlp internal linears map 512->1536, 512->1536, 1536->512

⸻

acceptance criteria
	•	all tests pass
	•	forward_with_attn_trace returns identical shapes/dtypes as before
	•	attention tracing returns pre-dropout softmax probabilities
	•	any ModelConfig() instantiations missing V are updated
	•	no changes to rope module
	•	no training loop logic changes beyond providing V to config
